import requests
import re
import time
import redis
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Optional, Dict
import random
from storage.logger import get_system_logger
import configparser
import os
import hmac
import hashlib
import base64
from urllib.parse import urlparse

import threading


class ProxyManager:
    """
    ä»£ç†æ± ç®¡ç†å™¨ï¼ˆRedisç‰ˆæœ¬ï¼‰

    ç‰¹æ€§ï¼š
    - RedisæŒä¹…åŒ–ä»£ç†
    - è‡ªåŠ¨éªŒè¯å’Œè¯„åˆ†
    - å¤±æ•ˆè‡ªåŠ¨ç§»é™¤
    - ä½äºé˜ˆå€¼è‡ªåŠ¨è¡¥å……
    - çº¿ç¨‹å®‰å…¨æ§åˆ¶
    """

    def __init__(
        self,
        redis_host="localhost",
        redis_port=6379,
        redis_password=None,
        redis_db=0,
        cache_key="guba:proxies:valid",
        target_url="https://guba.eastmoney.com/list,000001,1,f.html",
        min_threshold=5,
        target_count=20,
        max_count=30,
        context=None,
        config_path=None,
    ):
        """
        åˆå§‹åŒ–ä»£ç†ç®¡ç†å™¨
        """
        self.logger = get_system_logger()
        self.refill_lock = threading.Lock()  # è¡¥å……ä»£ç†æ—¶çš„é”ï¼Œé˜²æ­¢å¤šçº¿ç¨‹å¹¶å‘è§¦å‘

        # Redisè¿æ¥
        self.redis_client = redis.StrictRedis(
            host=redis_host,
            port=redis_port,
            password=redis_password if redis_password else None,
            db=redis_db,
            decode_responses=True,
        )

        self.cache_key = cache_key
        self.test_url = target_url
        self.timeout = 3
        self.min_threshold = min_threshold
        self.target_count = target_count
        self.max_count = max_count  # ä»£ç†æ± æœ€å¤§æ•°é‡
        self.context = context

        # è¯»å–é…ç½®æ–‡ä»¶
        if config_path is None:
            # é»˜è®¤ä»é¡¹ç›®æ ¹ç›®å½•è¯»å–config.ini
            current_dir = os.path.dirname(os.path.abspath(__file__))
            config_path = os.path.join(current_dir, "..", "config.ini")

        config = configparser.ConfigParser()
        config.read(config_path, encoding="utf-8")

        # è¯»å–ä»£ç†æä¾›å•†é…ç½®
        self.provider = "free"
        if config.has_section("proxies"):
            self.provider = config.get("proxies", "provider", fallback="free")
            # å…¼å®¹æ—§é…ç½®: use_paid_api=true -> qingguo
            if self.provider == "free" and config.getboolean(
                "proxies", "use_paid_api", fallback=False
            ):
                self.provider = "qingguo"

            # é’æœé…ç½®
            self.qingguo_url = config.get(
                "proxies", "qingguo_api_url", fallback="https://share.proxy.qg.net/get"
            )
            self.qingguo_key = config.get("proxies", "qingguo_api_key", fallback="")

            # KDLé…ç½®
            self.kdl_url = config.get(
                "proxies", "kdl_api_url", fallback="https://dps.kdlapi.com/api/getdps"
            )
            self.kdl_secret_id = config.get("proxies", "kdl_secret_id", fallback="")
            self.kdl_secret_key = config.get("proxies", "kdl_secret_key", fallback="")
            self.kdl_amount = config.getint("proxies", "kdl_amount", fallback=2)
            self.kdl_sign_type = config.get(
                "proxies", "kdl_sign_type", fallback="hmacsha1"
            )

            # è¯»å–ä»£ç†æ± æœ€å¤§å€¼é…ç½®
            self.max_count = config.getint(
                "proxies", "max_proxy_count", fallback=self.max_count
            )
            self.logger.info(f"âœ“ ä»£ç†æä¾›å•†: {self.provider}")
            self.logger.info(f"âœ“ ä»£ç†æ± æœ€å¤§å€¼é™åˆ¶: {self.max_count}")
        else:
            self.logger.info("âœ“ ä½¿ç”¨å…è´¹ä»£ç†æºæ¨¡å¼ï¼ˆæœªæ‰¾åˆ°proxiesé…ç½®ï¼‰")

        # å…è´¹ä»£ç†æºé…ç½®
        self.free_proxy_sources = [
            # 89ip APIï¼ˆæ–°ç‰ˆï¼‰
            {
                "type": "text",
                "url": "http://api.89ip.cn/tqdl.html?api=1&num=60&port=&address=&isp=",
                "name": "89ip-API",
            },
            # ProxyShare JSON API
            {
                "type": "json_list",
                "url": "https://www.proxyshare.com/web_v1/free-proxy/list?page_size=10&page=1&language=zh",
                "name": "ProxyShare",
            },
            # ProxyList JSON API
            {
                "type": "json_list",
                "url": "http://43.135.31.113:8777/proxyList?limit=50&page=1&language=zh-hans",
                "name": "ProxyList",
            },
            # proxy.scdn.io
            {
                "type": "text",
                "url": "https://proxy.scdn.io/text.php",
                "name": "proxy.scdn.io",
            },
            # GitHubå¼€æºåˆ—è¡¨
            {
                "type": "text",
                "url": "https://raw.githubusercontent.com/TheSpeedX/SOCKS-List/master/http.txt",
                "name": "GitHub-TheSpeedX",
            },
            {
                "type": "text",
                "url": "https://raw.githubusercontent.com/clarketm/proxy-list/master/proxy-list-raw.txt",
                "name": "GitHub-clarketm",
            },
        ]

    def count(self) -> int:
        """è·å–å½“å‰æœ‰æ•ˆä»£ç†æ•°é‡"""
        return self.redis_client.hlen(self.cache_key)

    def get_all(self) -> List[Dict]:
        """è·å–æ‰€æœ‰ä»£ç†"""
        proxies = []
        for proxy_url, score in self.redis_client.hgetall(self.cache_key).items():
            proxies.append({"proxy": proxy_url, "score": int(score)})

        # æŒ‰è¯„åˆ†æ’åº
        proxies.sort(key=lambda x: x["score"], reverse=True)
        return proxies

    def get_random_proxy(self) -> Optional[Dict]:
        """éšæœºè·å–ä¸€ä¸ªä»£ç†"""
        # æ£€æŸ¥é˜ˆå€¼ - åŒé‡æ£€æŸ¥é”å®š (Double Checked Locking)
        if self.count() < self.min_threshold:
            # å°è¯•è·å–é”ï¼Œåªæœ‰è·å–åˆ°é”çš„çº¿ç¨‹æ‰æ‰§è¡Œè¡¥å……ï¼Œå…¶ä»–çº¿ç¨‹ç­‰å¾…
            # è¿™é‡Œçš„é€»è¾‘æ˜¯ï¼šå¦‚æœç¼ºIPï¼Œå¤§å®¶éƒ½è¦åœä¸‹æ¥ç­‰è¡¥å……å®Œæˆ
            with self.refill_lock:
                # å†æ¬¡æ£€æŸ¥ï¼Œé˜²æ­¢åœ¨å‰ä¸€ä¸ªçº¿ç¨‹è¡¥å……å®Œä¹‹åï¼Œåç»­è·å–åˆ°é”çš„çº¿ç¨‹å†æ¬¡è¡¥å……
                if self.count() < self.min_threshold:
                    self.logger.info(f"âš ï¸ ä»£ç†æ± ä¸è¶³({self.count()}ä¸ª)ï¼Œè§¦å‘è‡ªåŠ¨è¡¥å……...")
                    self.refill_pool(target_count=self.target_count)

        proxies = self.get_all()
        if not proxies:
            return None

        # ä»é«˜åˆ†ä»£ç†ä¸­éšæœºé€‰æ‹©
        if len(proxies) > 10:
            top_half = proxies[: max(1, len(proxies) // 2)]
            selected = random.choice(top_half)
        else:
            selected = random.choice(proxies)

        proxy_url = selected["proxy"]
        return {"http": proxy_url, "https": proxy_url}

    def add_proxy(self, proxy_url: str, score: int = 100):
        """æ·»åŠ ä»£ç†åˆ°Redis"""
        import sys

        is_new = not self.redis_client.hexists(self.cache_key, proxy_url)
        self.redis_client.hset(self.cache_key, proxy_url, score)
        if is_new:
            total = self.count()
            # ç”¨æˆ·è¦æ±‚: error.logè®°å½•æ–°å¢IPè¯¦æƒ…
            # å†™å…¥stderrå¹¶flushï¼Œç¡®ä¿è¿›å…¥err.log (ç”±start.shå®šä¹‰)
            prefix = f"[{self.context}] " if self.context else ""
            sys.stderr.write(
                f"{prefix}â• [IPæ–°å¢] {proxy_url} (åˆ†å€¼:{score}, æ€»æ•°:{total})\n"
            )
            sys.stderr.flush()

    def remove_proxy(self, proxy_dict: Dict):
        """ç§»é™¤å¤±æ•ˆä»£ç†"""
        if not proxy_dict:
            return

        proxy_url = proxy_dict.get("http")
        if proxy_url:
            self.redis_client.hdel(self.cache_key, proxy_url)

    def update_score(self, proxy_url: str, success: bool):
        """æ›´æ–°ä»£ç†è¯„åˆ†"""
        current_score = self.redis_client.hget(self.cache_key, proxy_url)
        if current_score is None:
            return

        score = int(current_score)

        if success:
            score = min(100, score + 5)
        else:
            score = max(0, score - 10)

        if score < 30:
            # è¯„åˆ†è¿‡ä½ï¼Œç§»é™¤
            self.redis_client.hdel(self.cache_key, proxy_url)
        else:
            self.redis_client.hset(self.cache_key, proxy_url, score)

    def _generate_kdl_signature(self, method, url, params):
        """ç”ŸæˆKDLç­¾å"""
        # 1. è§£æURLè·å–path (e.g. /api/getdps)
        parsed_url = urlparse(url)
        path = parsed_url.path

        # 2. æ„é€ åŸæ–‡å­—ç¬¦ä¸²: METHOD + path + ? + sorted_query_string
        # æ³¨æ„ï¼šKDL demoæ˜¾ç¤ºpathå¯ä»¥ä¸å¸¦åŸŸåï¼Œè¿™é‡Œå°è¯•ä½¿ç”¨path
        s = f"{method.upper()}{path}?"

        # å‚æ•°æ’åºå¹¶æ‹¼æ¥
        query_parts = []
        for k in sorted(params.keys()):
            query_parts.append(f"{k}={params[k]}")
        query_str = "&".join(query_parts)

        raw_str = s + query_str

        # 3. HMAC-SHA1åŠ å¯†
        try:
            hmac_str = hmac.new(
                self.kdl_secret_key.encode("utf8"), raw_str.encode("utf8"), hashlib.sha1
            ).digest()
            signature = base64.b64encode(hmac_str).decode("utf-8")
            return signature
        except Exception as e:
            self.logger.error(f"KDLç­¾åç”Ÿæˆå¤±è´¥: {e}")
            return ""

    def _fetch_from_kdl(self, max_per_source: int) -> List[str]:
        """ä»KDLè·å–ä»£ç†"""
        raw_list = []
        self.logger.info("ğŸ“¡ [KDL] å¼€å§‹æå–ç§å¯†ä»£ç†...")

        # æ„é€ å‚æ•°
        params = {
            "secret_id": self.kdl_secret_id,
            "num": min(max_per_source, self.kdl_amount),
            "sign_type": self.kdl_sign_type,
            "timestamp": int(time.time()),
            "nonce": random.randint(100000, 999999),
            "format": "json",
            "sep": 1,
            "f_auth": 1,
            "generateType": 4,
        }

        # ç”Ÿæˆç­¾å
        signature = self._generate_kdl_signature("GET", self.kdl_url, params)
        params["signature"] = signature

        try:
            resp = requests.get(self.kdl_url, params=params, timeout=10)
            resp.raise_for_status()
            data = resp.json()

            if data.get("code") == 0:
                proxy_list = data.get("data", {}).get("proxy_list", [])
                raw_list.extend(proxy_list)
                self.logger.info(f"  âœ… [KDL] æˆåŠŸæå– {len(proxy_list)} ä¸ªä»£ç†")
            else:
                self.logger.error(
                    f"  âœ— [KDL] APIé”™è¯¯: {data.get('code')} - {data.get('msg')}"
                )

        except Exception as e:
            self.logger.error(f"  âœ— [KDL] è¯·æ±‚å¤±è´¥: {e}")

        return raw_list

    def _fetch_from_qingguo(self, max_per_source: int) -> List[str]:
        """ä»é’æœè·å–ä»£ç†"""
        raw_list = []
        self.logger.info("ğŸ“¡ [é’æœ] å¼€å§‹æå–ä»£ç†...")

        params = {
            "key": self.qingguo_key,
            "num": min(max_per_source, 5),
            "area": "",
            "isp": 0,
            "format": "json",
            "distinct": "true",
        }

        try:
            resp = requests.get(self.qingguo_url, params=params, timeout=10)
            data = resp.json()

            if data.get("code") == "SUCCESS":
                proxy_list = data.get("data", [])
                for p in proxy_list:
                    server = p.get("server")
                    if server:
                        raw_list.append(server)
                self.logger.info(f"  âœ… [é’æœ] æˆåŠŸæå– {len(raw_list)} ä¸ªä»£ç†")
            else:
                self.logger.error(f"  âœ— [é’æœ] APIé”™è¯¯: {data.get('message')}")
        except Exception as e:
            self.logger.error(f"  âœ— [é’æœ] è¯·æ±‚å¤±è´¥: {e}")

        return raw_list

    def _fetch_from_free(self) -> List[str]:
        """è·å–å…è´¹ä»£ç†"""
        raw_list = []
        self.logger.info("ğŸ“¡ [Free] å¼€å§‹æå–å…è´¹ä»£ç†...")

        headers = {"User-Agent": "Mozilla/5.0"}
        for source in self.free_proxy_sources:
            try:
                resp = requests.get(source["url"], headers=headers, timeout=10)
                if source["type"] == "json_list":
                    # ç®€å•JSONè§£æé€»è¾‘...
                    try:
                        data = resp.json()
                        items = data.get(
                            "data", data.get("list", data.get("proxies", []))
                        )
                        for item in items:
                            if isinstance(item, dict):
                                ip = item.get("ip", item.get("host"))
                                port = item.get("port")
                                if ip and port:
                                    raw_list.append(f"{ip}:{port}")
                    except:
                        pass
                else:
                    found = re.findall(r"\d+\.\d+\.\d+\.\d+[:ï¼š]\d+", resp.text)
                    raw_list.extend(found)
                self.logger.info(f"  âœ“ {source['name']}: {len(raw_list)} (cumulative)")
            except Exception as e:
                pass

        return raw_list

    def fetch_raw_ips(self, max_per_source: int = 100) -> List[str]:
        """ä»ä»£ç†æºæå–ä»£ç†ï¼ˆæ ¹æ®provideré…ç½®ï¼‰"""
        if self.provider == "kdl":
            return self._fetch_from_kdl(max_per_source)
        elif self.provider == "qingguo":
            return self._fetch_from_qingguo(max_per_source)
        else:
            return self._fetch_from_free()

    def verify_proxy(self, proxy_str: str) -> Optional[str]:
        """éªŒè¯ä»£ç†æ˜¯å¦å¯ç”¨"""
        proxy_url = proxy_str.replace("ï¼š", ":")
        if not proxy_url.startswith("http"):
            proxy_url = "http://" + proxy_url

        proxies = {"http": proxy_url, "https": proxy_url}

        try:
            print(f"[VERIFY] Testing proxy: {proxy_url}")
            start_time = time.time()
            resp = requests.get(
                self.test_url,
                proxies=proxies,
                timeout=self.timeout,
                headers={"User-Agent": "Mozilla/5.0"},
            )
            response_time = time.time() - start_time
            print(
                f"[VERIFY] {proxy_url} - Status: {resp.status_code}, Time: {response_time:.2f}s"
            )

            if resp.status_code == 200:
                # [Based on User Request] å¢åŠ å†…å®¹æ ¡éªŒé€»è¾‘
                # å¿…é¡»åŒ…å« article_list ä¸” count å€¼æ­£å¸¸
                content = resp.content.decode("utf-8", "ignore")

                if "var article_list" not in content:
                    print(
                        f"[VERIFY] {proxy_url} - FAIL: No 'var article_list' in content"
                    )
                    return None

                # [Refactor] æ ¡éªŒé€»è¾‘å˜æ›´ï¼šä¸å†æ£€æŸ¥count, è€Œæ˜¯æ£€æŸ¥user_nicknameåç¼€
                try:
                    import json

                    # content is already decoded string
                    start_index = content.find("var article_list")
                    start_json = content.find("{", start_index)

                    if start_json != -1:
                        decoder = json.JSONDecoder()
                        article_list_data, _ = decoder.raw_decode(content[start_json:])

                        items = article_list_data.get("re", [])
                        # å¦‚æœæ²¡æœ‰items, æš‚æ—¶è®¤ä¸ºå®ƒæ˜¯æœ‰æ•ˆçš„ï¼ˆå¯èƒ½æ˜¯å› ä¸ºæ²¡æœ‰æ•°æ®ï¼‰ï¼Œæˆ–è€…æ— æ•ˆï¼Ÿ
                        # åŸé€»è¾‘æ˜¯å¿…é¡»æœ‰countå­—æ®µã€‚è¿™é‡Œæˆ‘ä»¬è¿˜æ˜¯è¦æ±‚è§£ææˆåŠŸã€‚
                        # å¦‚æœæœ‰æ•°æ®ï¼Œå¿…é¡»æ»¡è¶³æ˜µç§°è§„åˆ™ã€‚
                        if items:
                            for item in items:
                                nickname = item.get("user_nickname", "")
                                if not nickname.endswith("èµ„è®¯"):
                                    print(
                                        f"[VERIFY] {proxy_url} - FAIL: Invalid nickname '{nickname}'"
                                    )
                                    return None

                        # ç¡®ä¿è§£ææ­£å¸¸
                        if "count" not in article_list_data:
                            print(
                                f"[VERIFY] {proxy_url} - FAIL: No 'count' field in article_list"
                            )
                            return None
                    else:
                        print(f"[VERIFY] {proxy_url} - FAIL: Cannot find JSON start")
                        return None

                except Exception as e:
                    print(f"[VERIFY] {proxy_url} - FAIL: JSON parse error: {e}")
                    return None

                score = max(100 - int(response_time * 20), 50)
                print(f"[VERIFY] {proxy_url} - SUCCESS! Score: {score}")
                return proxy_url, score
            else:
                print(f"[VERIFY] {proxy_url} - FAIL: HTTP {resp.status_code}")
        except requests.exceptions.Timeout:
            print(f"[VERIFY] {proxy_url} - FAIL: Timeout")
        except requests.exceptions.ProxyError as e:
            print(f"[VERIFY] {proxy_url} - FAIL: Proxy error: {e}")
        except requests.exceptions.ConnectionError as e:
            print(f"[VERIFY] {proxy_url} - FAIL: Connection error: {e}")
        except Exception as e:
            print(f"[VERIFY] {proxy_url} - FAIL: Unexpected error: {e}")
        return None

    def build_pool(self, max_workers: int = 30, max_per_source: int = 100):
        """åˆå§‹å»ºç«‹ä»£ç†æ± """
        all_raw_ips = []

        if self.provider == "kdl" or self.provider == "qingguo":
            # ä»˜è´¹APIæ¨¡å¼ï¼šéœ€è¦å¤šæ¬¡è°ƒç”¨
            calls_needed = (max_per_source + 4) // 5
            if self.provider == "kdl":
                # KDLæ¯æ¬¡å¯ä»¥å– amount ä¸ª, ç®€å•èµ·è§æŒ‰ amount ä¼°ç®—
                calls_needed = (max_per_source + self.kdl_amount - 1) // self.kdl_amount

            self.logger.info(
                f"ğŸ“Š é¢„è®¡éœ€è¦ {calls_needed} æ¬¡APIè°ƒç”¨ä»¥è·å– {max_per_source} ä¸ªä»£ç†"
            )

            for i in range(calls_needed):
                raw_ips = self.fetch_raw_ips(max_per_source=5)
                all_raw_ips.extend(raw_ips)

                # APIé™åˆ¶ï¼š60æ¬¡/åˆ†é’Ÿ
                if i < calls_needed - 1:
                    time.sleep(1.2)
        else:
            # å…è´¹ä»£ç†æ¨¡å¼ï¼šä¸€æ¬¡æ€§æŠ½å–
            all_raw_ips = self.fetch_raw_ips(max_per_source)

        self.logger.info(f"ğŸ” å¼€å§‹éªŒè¯ï¼ˆ{max_workers}çº¿ç¨‹ï¼‰...\n")

        valid_count = 0
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_proxy = {
                executor.submit(self.verify_proxy, ip): ip for ip in all_raw_ips
            }

            for future in as_completed(future_to_proxy):
                result = future.result()
                if result:
                    proxy_url, score = result
                    self.add_proxy(proxy_url, score)
                    valid_count += 1

                    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§å€¼
                    if self.count() >= self.max_count:
                        self.logger.info(
                            f"âš ï¸ å·²è¾¾åˆ°ä»£ç†æ± æœ€å¤§å€¼ ({self.max_count})ï¼Œåœæ­¢è·å–"
                        )
                        break

        self.logger.info(f"\nâœ… éªŒè¯å®Œæˆï¼Œè·å¾— {valid_count} ä¸ªæœ‰æ•ˆä»£ç†")
        return valid_count

    def refill_pool(self, target_count: int = 20, max_workers: int = 30):
        """è¡¥å……ä»£ç†æ± """
        print(
            f"[DEBUG] refill_pool called: target_count={target_count}, max_workers={max_workers}"
        )
        current = self.count()
        print(f"[DEBUG] current count: {current}, max_count: {self.max_count}")

        # æ£€æŸ¥æ˜¯å¦å·²è¾¾åˆ°æœ€å¤§å€¼
        if current >= self.max_count:
            self.logger.info(
                f"âœ… ä»£ç†æ± å·²è¾¾åˆ°æœ€å¤§å€¼ ({current}/{self.max_count})ï¼Œæ— éœ€è¡¥å……"
            )
            print(f"[DEBUG] Reached max count, returning")
            return

        self.logger.info(
            f"ğŸ”„ ä»£ç†æ± è¡¥å……ï¼ˆå½“å‰{current}ä¸ªï¼Œç›®æ ‡{target_count}ä¸ªï¼Œæœ€å¤§{self.max_count}ä¸ªï¼‰"
        )
        print(f"[DEBUG] Starting refill process")

        # è®¡ç®—éœ€è¦è¡¥å……çš„æ•°é‡ï¼Œä½†ä¸è¶…è¿‡æœ€å¤§å€¼
        needed = min(max(0, target_count - current), self.max_count - current)
        print(f"[DEBUG] needed: {needed}")
        if needed == 0:
            self.logger.info("âœ… ä»£ç†æ± å·²è¶³å¤Ÿï¼Œæ— éœ€è¡¥å……")
            print(f"[DEBUG] needed=0, returning")
            return

        all_raw_ips = []

        if self.provider == "kdl" or self.provider == "qingguo":
            # ä»˜è´¹APIæ¨¡å¼ï¼šå¤šæ¬¡è°ƒç”¨ç›´åˆ°è¾¾åˆ°ç›®æ ‡
            calls_needed = (needed + 4) // 5
            self.logger.info(
                f"ğŸ“Š éœ€è¦è¡¥å…… {needed} ä¸ªä»£ç†ï¼Œé¢„è®¡éœ€è¦ {calls_needed} æ¬¡APIè°ƒç”¨"
            )
            print(f"[DEBUG] Paid API mode: calls_needed={calls_needed}")

            for i in range(calls_needed):
                print(f"[DEBUG] API call {i + 1}/{calls_needed}")
                raw_ips = self.fetch_raw_ips(max_per_source=5)
                print(f"[DEBUG] Got {len(raw_ips)} raw IPs")
                all_raw_ips.extend(raw_ips)

                # å¦‚æœå·²ç»è·å–è¶³å¤Ÿçš„ä»£ç†ï¼Œæå‰é€€å‡º
                if len(all_raw_ips) >= needed:
                    break

                # APIé™åˆ¶ï¼š60æ¬¡/åˆ†é’Ÿ
                if i < calls_needed - 1:
                    time.sleep(1.2)
        else:
            # å…è´¹ä»£ç†æ¨¡å¼ï¼šä¸€æ¬¡æ€§æŠ½å–
            print(f"[DEBUG] Free proxy mode")
            all_raw_ips = self.fetch_raw_ips(max_per_source=100)
            print(f"[DEBUG] Got {len(all_raw_ips)} raw IPs")

        print(f"[DEBUG] Total raw IPs collected: {len(all_raw_ips)}")

        # è¿‡æ»¤å·²å­˜åœ¨çš„
        existing = set(self.redis_client.hkeys(self.cache_key))
        new_ips = [ip for ip in all_raw_ips if f"http://{ip}" not in existing]

        self.logger.info(f"ğŸ“Š è¿‡æ»¤å {len(new_ips)} ä¸ªæ–°å€™é€‰")
        print(f"[DEBUG] new_ips after filtering: {len(new_ips)}")

        added = 0
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_proxy = {
                executor.submit(self.verify_proxy, ip): ip for ip in new_ips
            }

            for future in as_completed(future_to_proxy):
                result = future.result()
                if result:
                    proxy_url, score = result
                    self.add_proxy(proxy_url, score)
                    added += 1

                    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡æˆ–æœ€å¤§å€¼
                    current_count = self.count()
                    if current_count >= target_count or current_count >= self.max_count:
                        if current_count >= self.max_count:
                            self.logger.info(
                                f"âš ï¸ å·²è¾¾åˆ°ä»£ç†æ± æœ€å¤§å€¼ ({self.max_count})ï¼Œåœæ­¢è¡¥å……"
                            )
                        break

        self.logger.info(f"âœ… è¡¥å……å®Œæˆï¼Œæ–°å¢{added}ä¸ªï¼Œå½“å‰å…±{self.count()}ä¸ª\n")
        print(f"[DEBUG] refill_pool completed: added {added}, total: {self.count()}")

    def revalidate_pool(self, max_workers: int = 20):
        """é‡æ–°éªŒè¯æ‰€æœ‰ä»£ç†"""
        proxies = self.get_all()
        self.logger.info(f"ğŸ”„ é‡æ–°éªŒè¯ {len(proxies)} ä¸ªä»£ç†...")

        # æ¸…ç©º
        self.redis_client.delete(self.cache_key)

        valid_count = 0
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_proxy = {
                executor.submit(self.verify_proxy, p["proxy"].replace("http://", "")): p
                for p in proxies
            }

            for future in as_completed(future_to_proxy):
                result = future.result()
                if result:
                    proxy_url, score = result
                    self.add_proxy(proxy_url, score)
                    valid_count += 1

        self.logger.info(f"âœ… éªŒè¯å®Œæˆï¼Œä¿ç•™ {valid_count} ä¸ªæœ‰æ•ˆä»£ç†\n")

    def start_maintenance_loop(self, check_interval: int = 300, min_threshold: int = 5):
        """
        å¯åŠ¨ä»£ç†æ± ç»´æŠ¤å¾ªç¯ï¼ˆå®ˆæŠ¤çº¿ç¨‹ï¼‰
        :param check_interval: æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰
        :param min_threshold: æœ€å°å¯ç”¨æ•°é‡é˜ˆå€¼
        """
        if hasattr(self, "_maintenance_thread") and self._maintenance_thread.is_alive():
            self.logger.warning("ä»£ç†æ± ç»´æŠ¤çº¿ç¨‹å·²åœ¨è¿è¡Œ")
            return

        self._running = True
        self._maintenance_thread = threading.Thread(
            target=self._maintenance_loop,
            args=(check_interval, min_threshold),
            name="ProxyMaintenanceThread",
            daemon=True,
        )
        self._maintenance_thread.start()
        self.logger.info("âœ“ ä»£ç†æ± ç»´æŠ¤çº¿ç¨‹å·²å¯åŠ¨")

    def stop_maintenance_loop(self):
        """åœæ­¢ç»´æŠ¤å¾ªç¯"""
        self._running = False

    def _maintenance_loop(self, check_interval, min_threshold):
        """ç»´æŠ¤å¾ªç¯å®ä½“"""
        self.logger.info(
            f"ä»£ç†æ± ç»´æŠ¤çº¿ç¨‹è¿è¡Œä¸­ (é˜ˆå€¼: {min_threshold}, é—´éš”: {check_interval}ç§’)"
        )

        # é¦–æ¬¡æ£€æŸ¥
        if self.count() < min_threshold:
            self.logger.info(f"é¦–æ¬¡æ£€æµ‹ä»£ç†ä¸è¶³({self.count()})ï¼Œæ‰§è¡Œåˆå§‹è¡¥å……...")
            self.build_pool(max_workers=50, max_per_source=200)

        while getattr(self, "_running", True):
            try:
                current_count = self.count()

                if current_count < min_threshold:
                    self.logger.warning(
                        f"âš ï¸ [è‡ªåŠ¨ç»´æŠ¤] ä»£ç†æ± ä¸è¶³: {current_count}/{min_threshold}ï¼Œå¼€å§‹è¡¥å……..."
                    )
                    # 1. é‡æ–°éªŒè¯ç°æœ‰
                    self.revalidate_pool()
                    # 2. å¦‚æœä»ä¸è¶³ï¼Œè¡¥å……
                    if self.count() < min_threshold:
                        self.refill_pool(target_count=self.target_count)
                        self.logger.info(
                            f"âœ“ [è‡ªåŠ¨ç»´æŠ¤] è¡¥å……å®Œæˆï¼Œå½“å‰å¯ç”¨: {self.count()}"
                        )
                else:
                    # self.logger.debug(f"[è‡ªåŠ¨ç»´æŠ¤] ä»£ç†æ± å¥åº· ({current_count}ä¸ª)")
                    pass

                time.sleep(check_interval)

            except Exception as e:
                self.logger.error(f"ä»£ç†æ± ç»´æŠ¤å¾ªç¯å¼‚å¸¸: {e}")
                time.sleep(60)

    # æ–‡ä»¶å­˜å‚¨ç›¸å…³æ–¹æ³•å·²ç§»é™¤ï¼Œå®Œå…¨ä½¿ç”¨Redisç®¡ç†


if __name__ == "__main__":
    # æµ‹è¯•
    print("=" * 60)
    print("ä»£ç†æ± ç®¡ç†å™¨æµ‹è¯•ï¼ˆRedisç‰ˆæœ¬ï¼‰")
    print("=" * 60)

    manager = ProxyManager()

    # æµ‹è¯•Redisè¿æ¥
    try:
        manager.redis_client.ping()
        print("âœ… Redisè¿æ¥æˆåŠŸ\n")
    except Exception:
        print("âŒ Redisè¿æ¥å¤±è´¥ï¼Œè¯·å¯åŠ¨RedisæœåŠ¡\n")
        exit(1)

    # å»ºç«‹ä»£ç†æ± 
    manager.build_pool(max_workers=30, max_per_source=50)

    # è·å–ä»£ç†
    print(f"\nå½“å‰ä»£ç†æ•°: {manager.count()}")
    proxy = manager.get_random_proxy()
    print(f"éšæœºä»£ç†: {proxy}")
