import requests
import re
import time
import redis
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Optional, Dict
import random
from storage.logger import get_system_logger

import threading


class ProxyManager:
    """
    ä»£ç†æ± ç®¡ç†å™¨ï¼ˆRedisç‰ˆæœ¬ï¼‰

    ç‰¹æ€§ï¼š
    - RedisæŒä¹…åŒ–ä»£ç†
    - è‡ªåŠ¨éªŒè¯å’Œè¯„åˆ†
    - å¤±æ•ˆè‡ªåŠ¨ç§»é™¤
    - ä½äºé˜ˆå€¼è‡ªåŠ¨è¡¥å……
    - çº¿ç¨‹å®‰å…¨æ§åˆ¶
    """

    def __init__(
        self,
        redis_host="localhost",
        redis_port=6379,
        redis_password=None,
        redis_db=0,
        cache_key="guba:proxies:valid",
        target_url="https://guba.eastmoney.com/list,000001,1,f.html",
        min_threshold=5,
        target_count=20,
        context=None,
    ):
        """
        åˆå§‹åŒ–ä»£ç†ç®¡ç†å™¨
        """
        self.logger = get_system_logger()
        self.refill_lock = threading.Lock()  # è¡¥å……ä»£ç†æ—¶çš„é”ï¼Œé˜²æ­¢å¤šçº¿ç¨‹å¹¶å‘è§¦å‘

        # Redisè¿æ¥
        self.redis_client = redis.StrictRedis(
            host=redis_host,
            port=redis_port,
            password=redis_password if redis_password else None,
            db=redis_db,
            decode_responses=True,
        )

        self.cache_key = cache_key
        self.test_url = target_url
        self.timeout = 3
        self.min_threshold = min_threshold
        self.target_count = target_count
        self.context = context

        # ä»£ç†æºé…ç½®
        self.sources = [
            # 89ip APIï¼ˆæ–°ç‰ˆï¼‰
            {
                "type": "text",
                "url": "http://api.89ip.cn/tqdl.html?api=1&num=60&port=&address=&isp=",
                "name": "89ip-API",
            },
            # ProxyShare JSON API
            {
                "type": "json_list",
                "url": "https://www.proxyshare.com/web_v1/free-proxy/list?page_size=10&page=1&language=zh",
                "name": "ProxyShare",
            },
            # ProxyList JSON API
            {
                "type": "json_list",
                "url": "http://43.135.31.113:8777/proxyList?limit=50&page=1&language=zh-hans",
                "name": "ProxyList",
            },
            # proxy.scdn.io
            {
                "type": "text",
                "url": "https://proxy.scdn.io/text.php",
                "name": "proxy.scdn.io",
            },
            # GitHubå¼€æºåˆ—è¡¨
            {
                "type": "text",
                "url": "https://raw.githubusercontent.com/TheSpeedX/SOCKS-List/master/http.txt",
                "name": "GitHub-TheSpeedX",
            },
            {
                "type": "text",
                "url": "https://raw.githubusercontent.com/clarketm/proxy-list/master/proxy-list-raw.txt",
                "name": "GitHub-clarketm",
            },
        ]

    def count(self) -> int:
        """è·å–å½“å‰æœ‰æ•ˆä»£ç†æ•°é‡"""
        return self.redis_client.hlen(self.cache_key)

    def get_all(self) -> List[Dict]:
        """è·å–æ‰€æœ‰ä»£ç†"""
        proxies = []
        for proxy_url, score in self.redis_client.hgetall(self.cache_key).items():
            proxies.append({"proxy": proxy_url, "score": int(score)})

        # æŒ‰è¯„åˆ†æ’åº
        proxies.sort(key=lambda x: x["score"], reverse=True)
        return proxies

    def get_random_proxy(self) -> Optional[Dict]:
        """éšæœºè·å–ä¸€ä¸ªä»£ç†"""
        # æ£€æŸ¥é˜ˆå€¼ - åŒé‡æ£€æŸ¥é”å®š (Double Checked Locking)
        if self.count() < self.min_threshold:
            # å°è¯•è·å–é”ï¼Œåªæœ‰è·å–åˆ°é”çš„çº¿ç¨‹æ‰æ‰§è¡Œè¡¥å……ï¼Œå…¶ä»–çº¿ç¨‹ç­‰å¾…
            # è¿™é‡Œçš„é€»è¾‘æ˜¯ï¼šå¦‚æœç¼ºIPï¼Œå¤§å®¶éƒ½è¦åœä¸‹æ¥ç­‰è¡¥å……å®Œæˆ
            with self.refill_lock:
                # å†æ¬¡æ£€æŸ¥ï¼Œé˜²æ­¢åœ¨å‰ä¸€ä¸ªçº¿ç¨‹è¡¥å……å®Œä¹‹åï¼Œåç»­è·å–åˆ°é”çš„çº¿ç¨‹å†æ¬¡è¡¥å……
                if self.count() < self.min_threshold:
                    self.logger.info(f"âš ï¸ ä»£ç†æ± ä¸è¶³({self.count()}ä¸ª)ï¼Œè§¦å‘è‡ªåŠ¨è¡¥å……...")
                    self.refill_pool(target_count=self.target_count)

        proxies = self.get_all()
        if not proxies:
            return None

        # ä»é«˜åˆ†ä»£ç†ä¸­éšæœºé€‰æ‹©
        if len(proxies) > 10:
            top_half = proxies[: max(1, len(proxies) // 2)]
            selected = random.choice(top_half)
        else:
            selected = random.choice(proxies)

        proxy_url = selected["proxy"]
        return {"http": proxy_url, "https": proxy_url}

    def add_proxy(self, proxy_url: str, score: int = 100):
        """æ·»åŠ ä»£ç†åˆ°Redis"""
        import sys

        is_new = not self.redis_client.hexists(self.cache_key, proxy_url)
        self.redis_client.hset(self.cache_key, proxy_url, score)
        if is_new:
            total = self.count()
            # ç”¨æˆ·è¦æ±‚: error.logè®°å½•æ–°å¢IPè¯¦æƒ…
            # å†™å…¥stderrå¹¶flushï¼Œç¡®ä¿è¿›å…¥err.log (ç”±start.shå®šä¹‰)
            prefix = f"[{self.context}] " if self.context else ""
            sys.stderr.write(
                f"{prefix}â• [IPæ–°å¢] {proxy_url} (åˆ†å€¼:{score}, æ€»æ•°:{total})\n"
            )
            sys.stderr.flush()

    def remove_proxy(self, proxy_dict: Dict):
        """ç§»é™¤å¤±æ•ˆä»£ç†"""
        if not proxy_dict:
            return

        proxy_url = proxy_dict.get("http")
        if proxy_url:
            self.redis_client.hdel(self.cache_key, proxy_url)

    def update_score(self, proxy_url: str, success: bool):
        """æ›´æ–°ä»£ç†è¯„åˆ†"""
        current_score = self.redis_client.hget(self.cache_key, proxy_url)
        if current_score is None:
            return

        score = int(current_score)

        if success:
            score = min(100, score + 5)
        else:
            score = max(0, score - 10)

        if score < 30:
            # è¯„åˆ†è¿‡ä½ï¼Œç§»é™¤
            self.redis_client.hdel(self.cache_key, proxy_url)
        else:
            self.redis_client.hset(self.cache_key, proxy_url, score)

    def fetch_raw_ips(self, max_per_source: int = 100) -> List[str]:
        """ä»æºç«™æŠ“å–åŸå§‹IPåˆ—è¡¨"""
        raw_list = []
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }

        self.logger.info("ğŸ“¡ å¼€å§‹æŠ“å–ä»£ç†æº...")
        for source in self.sources:
            try:
                url = source["url"]
                name = source["name"]
                source_type = source["type"]
                params = source.get("params", {})

                if source_type == "zdaili_api":
                    resp = requests.get(url, params=params, headers=headers, timeout=10)
                    time.sleep(10)
                else:
                    resp = requests.get(url, headers=headers, timeout=10)

                # è§£æå“åº”
                if source_type == "zdaili_api":
                    try:
                        data = resp.json()
                        code = str(data.get("code"))
                        if code == "10001":
                            proxy_list = data.get("data", {}).get("proxy_list", [])
                            for proxy_info in proxy_list:
                                ip = proxy_info.get("ip")
                                port = proxy_info.get("port")
                                if ip and port:
                                    raw_list.append(f"{ip}:{port}")
                            self.logger.info(f"  âœ“ {name}: {len(proxy_list)}ä¸ª")
                        else:
                            self.logger.warning(f"  âœ— {name}: {data.get('msg')}")
                    except:
                        pass
                else:
                    # æ–‡æœ¬æ ¼å¼
                    found = re.findall(r"\d+\.\d+\.\d+\.\d+[:ï¼š]\d+", resp.text)
                    raw_list.extend(found)
                    self.logger.info(f"  âœ“ {name}: {len(found)}ä¸ª")

            except Exception as e:
                self.logger.warning(f"  âœ— {source['name']}: {e}")

        unique_list = list(set(raw_list))
        if not unique_list:
            self.logger.warning(
                "âš ï¸ è­¦å‘Š: ä»æ‰€æœ‰æºè·å–åˆ°çš„IPæ•°é‡ä¸º0ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–æºç«™å¯ç”¨æ€§"
            )
        self.logger.info(f"ğŸ“Š å…±æŠ“å– {len(unique_list)} ä¸ªå”¯ä¸€ä»£ç†\n")
        return unique_list

    def verify_proxy(self, proxy_str: str) -> Optional[str]:
        """éªŒè¯ä»£ç†æ˜¯å¦å¯ç”¨"""
        proxy_url = proxy_str.replace("ï¼š", ":")
        if not proxy_url.startswith("http"):
            proxy_url = "http://" + proxy_url

        proxies = {"http": proxy_url, "https": proxy_url}

        try:
            start_time = time.time()
            resp = requests.get(
                self.test_url,
                proxies=proxies,
                timeout=self.timeout,
                headers={"User-Agent": "Mozilla/5.0"},
            )
            response_time = time.time() - start_time

            if resp.status_code == 200:
                # [Based on User Request] å¢åŠ å†…å®¹æ ¡éªŒé€»è¾‘
                # å¿…é¡»åŒ…å« article_list ä¸” count å€¼æ­£å¸¸
                content = resp.content.decode("utf-8", "ignore")

                if "var article_list" not in content:
                    return None

                # [Refactor] æ ¡éªŒé€»è¾‘å˜æ›´ï¼šä¸å†æ£€æŸ¥count, è€Œæ˜¯æ£€æŸ¥user_nicknameåç¼€
                try:
                    import json

                    # content is already decoded string
                    start_index = content.find("var article_list")
                    start_json = content.find("{", start_index)

                    if start_json != -1:
                        decoder = json.JSONDecoder()
                        article_list_data, _ = decoder.raw_decode(content[start_json:])

                        items = article_list_data.get("re", [])
                        # å¦‚æœæ²¡æœ‰items, æš‚æ—¶è®¤ä¸ºå®ƒæ˜¯æœ‰æ•ˆçš„ï¼ˆå¯èƒ½æ˜¯å› ä¸ºæ²¡æœ‰æ•°æ®ï¼‰ï¼Œæˆ–è€…æ— æ•ˆï¼Ÿ
                        # åŸé€»è¾‘æ˜¯å¿…é¡»æœ‰countå­—æ®µã€‚è¿™é‡Œæˆ‘ä»¬è¿˜æ˜¯è¦æ±‚è§£ææˆåŠŸã€‚
                        # å¦‚æœæœ‰æ•°æ®ï¼Œå¿…é¡»æ»¡è¶³æ˜µç§°è§„åˆ™ã€‚
                        if items:
                            for item in items:
                                nickname = item.get("user_nickname", "")
                                if not nickname.endswith("èµ„è®¯"):
                                    # self.logger.debug(f"âš ï¸ {proxy_url} è¿”å›å¼‚å¸¸æ˜µç§° ({nickname})")
                                    return None

                        # ç¡®ä¿è§£ææ­£å¸¸
                        if "count" not in article_list_data:
                            return None
                    else:
                        return None

                except Exception:
                    return None

                score = max(100 - int(response_time * 20), 50)
                # self.logger.debug(f" âœ“ {proxy_url} (å“åº”{response_time:.2f}s, è¯„åˆ†{score})")
                return proxy_url, score
        except:
            pass

        return None

    def build_pool(self, max_workers: int = 30, max_per_source: int = 100):
        """åˆå§‹å»ºç«‹ä»£ç†æ± """
        raw_ips = self.fetch_raw_ips(max_per_source)
        self.logger.info(f"ğŸ” å¼€å§‹éªŒè¯ï¼ˆ{max_workers}çº¿ç¨‹ï¼‰...\n")

        valid_count = 0
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_proxy = {
                executor.submit(self.verify_proxy, ip): ip for ip in raw_ips
            }

            for future in as_completed(future_to_proxy):
                result = future.result()
                if result:
                    proxy_url, score = result
                    self.add_proxy(proxy_url, score)
                    valid_count += 1

        self.logger.info(f"\nâœ… éªŒè¯å®Œæˆï¼Œè·å¾— {valid_count} ä¸ªæœ‰æ•ˆä»£ç†")
        return valid_count

    def refill_pool(self, target_count: int = 20, max_workers: int = 30):
        """è¡¥å……ä»£ç†æ± """
        current = self.count()
        self.logger.info(f"ğŸ”„ ä»£ç†æ± è¡¥å……ï¼ˆå½“å‰{current}ä¸ªï¼Œç›®æ ‡{target_count}ä¸ªï¼‰")

        raw_ips = self.fetch_raw_ips(max_per_source=100)

        # è¿‡æ»¤å·²å­˜åœ¨çš„
        existing = set(self.redis_client.hkeys(self.cache_key))
        new_ips = [ip for ip in raw_ips if f"http://{ip}" not in existing]

        self.logger.info(f"ğŸ“Š è¿‡æ»¤å {len(new_ips)} ä¸ªæ–°å€™é€‰")

        added = 0
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_proxy = {
                executor.submit(self.verify_proxy, ip): ip for ip in new_ips
            }

            for future in as_completed(future_to_proxy):
                result = future.result()
                if result:
                    proxy_url, score = result
                    self.add_proxy(proxy_url, score)
                    added += 1

                    if self.count() >= target_count:
                        break

        self.logger.info(f"âœ… è¡¥å……å®Œæˆï¼Œæ–°å¢{added}ä¸ªï¼Œå½“å‰å…±{self.count()}ä¸ª\n")

    def revalidate_pool(self, max_workers: int = 20):
        """é‡æ–°éªŒè¯æ‰€æœ‰ä»£ç†"""
        proxies = self.get_all()
        self.logger.info(f"ğŸ”„ é‡æ–°éªŒè¯ {len(proxies)} ä¸ªä»£ç†...")

        # æ¸…ç©º
        self.redis_client.delete(self.cache_key)

        valid_count = 0
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_proxy = {
                executor.submit(self.verify_proxy, p["proxy"].replace("http://", "")): p
                for p in proxies
            }

            for future in as_completed(future_to_proxy):
                result = future.result()
                if result:
                    proxy_url, score = result
                    self.add_proxy(proxy_url, score)
                    valid_count += 1

        self.logger.info(f"âœ… éªŒè¯å®Œæˆï¼Œä¿ç•™ {valid_count} ä¸ªæœ‰æ•ˆä»£ç†\n")

    def start_maintenance_loop(self, check_interval: int = 300, min_threshold: int = 5):
        """
        å¯åŠ¨ä»£ç†æ± ç»´æŠ¤å¾ªç¯ï¼ˆå®ˆæŠ¤çº¿ç¨‹ï¼‰
        :param check_interval: æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰
        :param min_threshold: æœ€å°å¯ç”¨æ•°é‡é˜ˆå€¼
        """
        if hasattr(self, "_maintenance_thread") and self._maintenance_thread.is_alive():
            self.logger.warning("ä»£ç†æ± ç»´æŠ¤çº¿ç¨‹å·²åœ¨è¿è¡Œ")
            return

        self._running = True
        self._maintenance_thread = threading.Thread(
            target=self._maintenance_loop,
            args=(check_interval, min_threshold),
            name="ProxyMaintenanceThread",
            daemon=True,
        )
        self._maintenance_thread.start()
        self.logger.info("âœ“ ä»£ç†æ± ç»´æŠ¤çº¿ç¨‹å·²å¯åŠ¨")

    def stop_maintenance_loop(self):
        """åœæ­¢ç»´æŠ¤å¾ªç¯"""
        self._running = False

    def _maintenance_loop(self, check_interval, min_threshold):
        """ç»´æŠ¤å¾ªç¯å®ä½“"""
        self.logger.info(
            f"ä»£ç†æ± ç»´æŠ¤çº¿ç¨‹è¿è¡Œä¸­ (é˜ˆå€¼: {min_threshold}, é—´éš”: {check_interval}ç§’)"
        )

        # é¦–æ¬¡æ£€æŸ¥
        if self.count() < min_threshold:
            self.logger.info(f"é¦–æ¬¡æ£€æµ‹ä»£ç†ä¸è¶³({self.count()})ï¼Œæ‰§è¡Œåˆå§‹è¡¥å……...")
            self.build_pool(max_workers=50, max_per_source=200)

        while getattr(self, "_running", True):
            try:
                current_count = self.count()

                if current_count < min_threshold:
                    self.logger.warning(
                        f"âš ï¸ [è‡ªåŠ¨ç»´æŠ¤] ä»£ç†æ± ä¸è¶³: {current_count}/{min_threshold}ï¼Œå¼€å§‹è¡¥å……..."
                    )
                    # 1. é‡æ–°éªŒè¯ç°æœ‰
                    self.revalidate_pool()
                    # 2. å¦‚æœä»ä¸è¶³ï¼Œè¡¥å……
                    if self.count() < min_threshold:
                        self.refill_pool(target_count=self.target_count)
                        self.logger.info(
                            f"âœ“ [è‡ªåŠ¨ç»´æŠ¤] è¡¥å……å®Œæˆï¼Œå½“å‰å¯ç”¨: {self.count()}"
                        )
                else:
                    # self.logger.debug(f"[è‡ªåŠ¨ç»´æŠ¤] ä»£ç†æ± å¥åº· ({current_count}ä¸ª)")
                    pass

                time.sleep(check_interval)

            except Exception as e:
                self.logger.error(f"ä»£ç†æ± ç»´æŠ¤å¾ªç¯å¼‚å¸¸: {e}")
                time.sleep(60)

    # æ–‡ä»¶å­˜å‚¨ç›¸å…³æ–¹æ³•å·²ç§»é™¤ï¼Œå®Œå…¨ä½¿ç”¨Redisç®¡ç†


if __name__ == "__main__":
    # æµ‹è¯•
    print("=" * 60)
    print("ä»£ç†æ± ç®¡ç†å™¨æµ‹è¯•ï¼ˆRedisç‰ˆæœ¬ï¼‰")
    print("=" * 60)

    manager = ProxyManager()

    # æµ‹è¯•Redisè¿æ¥
    try:
        manager.redis_client.ping()
        print("âœ… Redisè¿æ¥æˆåŠŸ\n")
    except:
        print("âŒ Redisè¿æ¥å¤±è´¥ï¼Œè¯·å¯åŠ¨RedisæœåŠ¡\n")
        exit(1)

    # å»ºç«‹ä»£ç†æ± 
    manager.build_pool(max_workers=30, max_per_source=50)

    # è·å–ä»£ç†
    print(f"\nå½“å‰ä»£ç†æ•°: {manager.count()}")
    proxy = manager.get_random_proxy()
    print(f"éšæœºä»£ç†: {proxy}")
