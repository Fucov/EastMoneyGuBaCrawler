# -*- coding: utf-8 -*-
# @Time    : 2023/2/11 21:27
# @Author  : Euclid-Jie
# @File    : main_class.py
import pandas as pd
import requests
from bs4 import BeautifulSoup
from tqdm import tqdm
import logging
from datetime import datetime

# from retrying import retry
from concurrent.futures import ThreadPoolExecutor
# from Utils.MongoClient import MongoClient
# from Utils.EuclidDataTools import CsvClient
# from TreadCrawler import RedisClient

import configparser
from storage.database_client import DatabaseManager
from core.user_agent_manager import get_user_agent_manager
import time
import random

import sys

from tenacity import (
    retry,
    retry_if_exception_type,
    stop_after_attempt,
    wait_exponential,
)
from pymongo.errors import BulkWriteError
from core.proxy_manager import ProxyManager  # ä½¿ç”¨ç»Ÿä¸€çš„ä»£ç†æ± ç®¡ç†å™¨


# å®šä¹‰è‡ªå®šä¹‰å¼‚å¸¸ç±»
class NetworkException(Exception):
    pass


class ServerException(Exception):
    pass


class ContentChangedException(Exception):
    pass


class guba_comments:
    """
    this class is designed for get hot comments for guba, have two method which can be set at def get_data()
    1ã€all: https://guba.eastmoney.com/list,600519_1.html, secCode: 600519, page: 1
    2ã€hot: https://guba.eastmoney.com/list,600519,99_1.html secCode: 600519, page: 1

    because to the ip control, this need to set proxies pools
    by using proxies https://www.kuaidaili.com/usercenter/overview/, can solve this problem

    Program characteristics:
        1ã€default write data to mongoDB, by init "MogoDB=False", can switch to write data to csv file
        2ã€Use retry mechanism, once rise error, the program will restart at the least page and num (each page has 80 num)

    """

    failed_proxies = {}
    proxy_fail_times_treshold = 3

    def __init__(
        self,
        config_path: str,
        config: configparser.ConfigParser,
        secCode,
        pages_start: int = 0,
        pages_end: int = 100,
        num_start: int = 0,
        MongoDB: bool = True,
        collectionName: str = "default",
        full_text: bool = False,
        max_workers: int = 8,  # æ–°å¢ï¼šå¤šçº¿ç¨‹å¹¶å‘æ•°
    ):
        self.config_path = config_path
        self.db_manager = DatabaseManager(self.config_path)

        # å‚æ•°åˆå§‹åŒ– - ä¿®å¤secCodeæœªèµ‹å€¼çš„bug
        if isinstance(secCode, int):
            # è¡¥é½6ä½æ•°
            self.secCode = str(secCode).zfill(6)
        elif isinstance(secCode, str):
            self.secCode = secCode
        else:
            self.secCode = str(secCode)

        self.pages_start = pages_start
        self.pages_end = pages_end
        self.num_start = num_start
        self.full_text = full_text
        self._year = pd.Timestamp.now().year

        # å¹´ä»½æ¨æ–­ï¼šç”¨äºå¤„ç†publish_timeæ²¡æœ‰å¹´ä»½çš„é—®é¢˜
        self.current_year = datetime.now().year
        self.last_month = None  # è®°å½•ä¸Šä¸€æ¡æ–°é—»çš„æœˆä»½ï¼Œç”¨äºæ¨æ–­å¹´ä»½

        # å¢é‡æ›´æ–°ä¼˜åŒ–ï¼šé¡µçº§é‡å¤è®¡æ•°å™¨ï¼ˆæ£€æµ‹è¿ç»­é‡å¤é¡µé¢ï¼‰
        self.consecutive_duplicate_pages = 0
        self.duplicate_page_threshold = 2  # è¿ç»­2é¡µå…¨ä¸ºé‡å¤åˆ™è·³è¿‡è¯¥æ ç›®

        # å¤šçº¿ç¨‹é…ç½®
        self.max_workers = max_workers

        # redis client for full_text_Crawler
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨Redis
        self.redis_enabled = config.getboolean("Redis", "enabled", fallback=True)
        if self.redis_enabled:
            self.redis_client = self.db_manager.get_redis_client()
        else:
            self.redis_client = None

        # é…ç½®æ–‡ä»¶å¯ä»¥è¦†ç›–å‚æ•°
        if config.has_option("mainClass", "secCode"):
            self.secCode = config.get("mainClass", "secCode")
            print(
                f"secCode has been overridden by {self.secCode} in the configuration file."
            )
        if config.has_option("mainClass", "pages_start"):
            self.pages_start = int(config.get("mainClass", "pages_start"))
            print(
                f"pages_start has been overridden by {self.pages_start} in the configuration file."
            )
        if config.has_option("mainClass", "pages_end"):
            self.pages_end = int(config.get("mainClass", "pages_end"))
            print(
                f"pages_end has been overridden by {self.pages_end} in the configuration file."
            )
        if config.has_option("mainClass", "collectionname"):
            collectionName = config.get("mainClass", "collectionname")
            print(
                f"collectionName has been overridden by {collectionName} in the configuration file."
            )
        if config.has_option("mainClass", "max_workers"):
            self.max_workers = int(config.get("mainClass", "max_workers"))
            print(
                f"max_workers has been overridden by {self.max_workers} in the configuration file."
            )

        # choose one save method, default MongoDB
        # 1ã€csv
        # 2ã€MongoDB
        # å¸–å­æ•°æ®å­˜å‚¨åˆ°é…ç½®çš„æ•°æ®åº“
        # self.log_path = config.get("logging", "log_file")

        # print("è¿™æ˜¯è¯»å–åˆ°æ ‡é¢˜çˆ¬å–çš„log"+self.log_path)

        # ä»é…ç½®æ–‡ä»¶è¯»å–æ•°æ®åº“åç§°å’ŒCollectionåç§°
        db_name = config.get("MongoDB", "database", fallback="guba")
        # ä½¿ç”¨ç»Ÿä¸€çš„Collectionåç§°ï¼šstock_newsï¼ˆæ‰€æœ‰è‚¡ç¥¨å…±ç”¨ï¼‰
        collection_name = config.get(
            "mainClass", "collectionName", fallback="stock_news"
        )
        # ä½¿ç”¨ database_client çš„ MongoDB å®¢æˆ·ç«¯
        self.col = self.db_manager.get_mongo_client(db_name, collection_name).collection
        self.collection_name = collection_name  # ä¿å­˜Collectionåç§°ä¾›åç»­ä½¿ç”¨

        # log setting
        # log_format = "%(levelname)s %(asctime)s %(filename)s %(lineno)d %(message)s"
        # logging.basicConfig(filename=self.log_path, format=log_format, level=logging.INFO,encoding = 'utf-8')

        # è®¾ç½®æ—¥å¿—çº§åˆ«
        log_level_str = config.get("logging", "log_level", fallback="INFO").upper()
        log_level = getattr(logging, log_level_str, logging.INFO)

        self.logger = logging.getLogger(
            config.get("logging", "log_file", fallback="main_controller.log")
        )
        if not self.logger.handlers:
            self.logger.setLevel(log_level)
            # ä½¿ç”¨ sys.stdout ç¡®ä¿æ—¥å¿—è¾“å‡ºåˆ° run.log (å› ä¸º start.sh ä¸­ > run.log)
            handler = logging.StreamHandler(sys.stdout)
            handler.setLevel(log_level)
            # è®¾ç½®è¯¥handlerçš„æ ¼å¼
            formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)

        # User-Agentç®¡ç†å™¨åˆå§‹åŒ–
        self.ua_manager = get_user_agent_manager("random")  # ä½¿ç”¨éšæœºæ¨¡å¼

        # header setting - ä½¿ç”¨åŠ¨æ€User-Agent
        self.header = {
            "User-Agent": self.ua_manager.get_user_agent(),
        }

        # ä»£ç†é…ç½® - ç»Ÿä¸€ä» [proxies] éƒ¨åˆ†è¯»å–
        proxy_enabled = config.getboolean("proxies", "enabled", fallback=False)

        # åˆå§‹åŒ–ä»£ç†æ± 
        self.proxy_pool = None
        if proxy_enabled:
            print("\nğŸ”§ åˆå§‹åŒ–ä»£ç†æ± ...")
            # ä»é…ç½®è¯»å–ä»£ç†æ± å‚æ•°
            min_proxy_threshold = config.getint(
                "proxies", "min_proxy_count", fallback=5
            )
            target_proxy_count = config.getint("proxies", "target_count", fallback=20)

            self.proxy_pool = ProxyManager(
                redis_host=config.get("Redis", "host", fallback="localhost"),
                redis_port=config.getint("Redis", "port", fallback=6379),
                redis_password=config.get("Redis", "password", fallback="") or None,
                redis_db=config.getint("Redis", "db", fallback=0),
                cache_key=config.get(
                    "Redis", "proxy_cache_key", fallback="guba:proxies:valid"
                ),
                min_threshold=min_proxy_threshold,
                target_count=target_proxy_count,
                context=self.secCode,
                config_path=self.config_path,  # ä¼ é€’é…ç½®æ–‡ä»¶è·¯å¾„
            )

            # æ£€æŸ¥ä»£ç†æ± çŠ¶æ€
            if self.proxy_pool.count() == 0:
                print("âš ï¸ ä»£ç†æ± ä¸ºç©ºï¼Œå°è¯•ä½¿ç”¨ç°æœ‰ä»£ç†æˆ–ç­‰å¾…è°ƒåº¦å™¨è¡¥å……...")
            else:
                current_count = self.proxy_pool.count()
                if self.proxy_pool.provider == "kdl":
                    mode = "å¿«ä»£ç†(KDL)"
                elif self.proxy_pool.provider == "qingguo":
                    mode = "é’æœä»£ç†"
                else:
                    mode = "å…è´¹ä»£ç†æº"
                print(f"âœ… ä»£ç†æ± å°±ç»ª (æ¨¡å¼: {mode}, å½“å‰å¯ç”¨: {current_count})")
            print("")

            self.proxies = None  # ä½¿ç”¨ä»£ç†æ± è€Œéå›ºå®šä»£ç†
            self.backup_proxies = None
        else:
            self.proxies = None
            self.backup_proxies = None
            print("âš ï¸ ä»£ç†å·²ç¦ç”¨ - ç›´æ¥è¿æ¥")

        self.use_backup_proxy = False

    @staticmethod
    def clear_str(str_raw):
        for pat in ["\n", " ", " ", "\r", "\xa0", "\n\r\n"]:
            str_raw.strip(pat).replace(pat, "")
        return str_raw

    @staticmethod
    def run_thread_pool_sub(target, args, max_work_count):
        with ThreadPoolExecutor(max_workers=max_work_count) as t:
            res = [t.submit(target, i) for i in args]
            return res

    def get_total_pages(self, content_type="news"):
        """
        è‡ªåŠ¨æ£€æµ‹æŒ‡å®šå†…å®¹ç±»å‹çš„æ€»é¡µæ•°

        é€šè¿‡è§£æé¡µé¢ä¸­çš„JavaScriptå˜é‡ var article_list = {...}
        æå– count å­—æ®µï¼ˆæ€»å¸–å­æ•°ï¼‰ï¼Œç„¶åè®¡ç®—æ€»é¡µæ•° = ceil(count / 80)

        Args:
            content_type: 'news' (èµ„è®¯) | 'report' (ç ”æŠ¥) | 'notice' (å…¬å‘Š)

        Returns:
            tuple: (total_pages, total_count)
            int: æ€»é¡µæ•°ï¼Œå¦‚æœæ£€æµ‹å¤±è´¥è¿”å›0
            int: æ€»æ¡æ•°ï¼Œå¦‚æœæ£€æµ‹å¤±è´¥è¿”å›0
        """
        import math
        # import re  <-- Removed unused import

        # ç±»å‹æ˜ å°„
        type_map = {"news": "1,f", "report": "2,f", "notice": "3,f"}

        if content_type not in type_map:
            self.logger.error(f"æœªçŸ¥çš„å†…å®¹ç±»å‹: {content_type}")
            return 0, 0

        # æ„é€ ç¬¬ä¸€é¡µURL
        url = f"https://guba.eastmoney.com/list,{self.secCode},{type_map[content_type]}.html"

        max_retries = 5  # User requested 5 retries for this critical step
        for attempt in range(max_retries):
            try:
                soup, proxy_used = self.get_soup_form_url(url)
                if not soup:
                    if attempt < max_retries - 1:
                        time.sleep(random.uniform(1, 3))
                        continue
                    return 0, 0

                # æŸ¥æ‰¾æ‰€æœ‰scriptæ ‡ç­¾
                scripts = soup.find_all("script")

                # Flag to track if we found a "bad" result that necessitates a force retry
                force_retry = False

                for script in scripts:
                    script_text = script.string
                    if script_text and "var article_list" in script_text:
                        # æ‰¾åˆ°åŒ…å« article_list çš„è„šæœ¬
                        # [Refactor] æ ¡éªŒé€»è¾‘å˜æ›´ï¼šä¸å†æ£€æŸ¥count, è€Œæ˜¯æ£€æŸ¥user_nicknameåç¼€
                        try:
                            import json

                            # Use raw_decode to parse the JSON object starting from '{'
                            start_index = script_text.find("{")
                            if start_index != -1:
                                decoder = json.JSONDecoder()
                                article_list_data, _ = decoder.raw_decode(
                                    script_text[start_index:]
                                )

                                # Extract items list
                                items = article_list_data.get("re", [])
                                if items:
                                    # Check if ALL items have user_nickname ending with "èµ„è®¯"
                                    # Note: We need to be careful if the list is empty, but if it has items, check them.
                                    # If list is empty, maybe it's valid (no news), or maybe invalid.
                                    # Assuming if we get data, we validate it.

                                    invalid_nicknames = []
                                    for item in items:
                                        nickname = item.get("user_nickname", "")
                                        if not nickname.endswith("èµ„è®¯"):
                                            invalid_nicknames.append(nickname)

                                    if invalid_nicknames:
                                        print(
                                            f"âš ï¸ {content_type}: å‘ç°é'èµ„è®¯'ç»“å°¾çš„æ˜µç§° ({invalid_nicknames[:3]}...), å¯èƒ½è¢«åçˆ¬è™«ï¼Œæ­£åœ¨é‡è¯•(Attempt {attempt + 1}/{max_retries})..."
                                        )
                                        # [Repetitive Logic] Penalize IP
                                        if (
                                            self.proxy_pool
                                            and proxy_used
                                            and proxy_used.startswith("http")
                                        ):
                                            self.logger.info(
                                                f"ğŸš« å¯¹IP {proxy_used} è¿›è¡Œæ‰£åˆ†å¤„ç†"
                                            )
                                            self.proxy_pool.update_score(
                                                proxy_used, success=False
                                            )

                                        force_retry = True
                                        break  # Break from script loop to trigger outer retry

                                # extracting count for total_pages calculation
                                total_count = int(article_list_data.get("count", 0))

                                # æ¯é¡µ80æ¡ï¼Œè®¡ç®—æ€»é¡µæ•°
                                total_pages = math.ceil(total_count / 80)
                                print(
                                    f"âœ“ {content_type}: æ ¡éªŒé€šè¿‡ (æ˜µç§°åç¼€æ£€æŸ¥)ï¼Œå…±{total_count}æ¡æ•°æ®ï¼Œ{total_pages}é¡µ"
                                )
                                return total_pages, total_count

                        except Exception as e:
                            print(f"âš ï¸ {content_type}: JSONè§£ææˆ–æ ¡éªŒå¤±è´¥: {e}")
                            continue

                # Logic for retry:
                # If we are here, it means either:
                # 1. No script contained 'article_list'
                # 2. 'article_list' was found but count was > 50000 (force_retry=True)

                if force_retry:
                    # Log already printed above
                    pass
                else:
                    print(
                        f"âš ï¸ {content_type}: æœªæ‰¾åˆ°article_listå˜é‡ (Attempt {attempt + 1}/{max_retries})"
                    )

                if attempt < max_retries - 1:
                    time.sleep(random.uniform(1, 3))
                    continue
                else:
                    print(f"âš ï¸ {content_type}: å¤šæ¬¡é‡è¯•åå‡å¤±è´¥ï¼Œé»˜è®¤1é¡µ")
                    return 1, 0

            except Exception as e:
                print(f"âŒ {content_type}: æ£€æµ‹é¡µæ•°å¼‚å¸¸(ç¬¬{attempt + 1}æ¬¡) - {e}")
                if attempt < max_retries - 1:
                    time.sleep(random.uniform(1, 3))
                else:
                    self.logger.error(f"æ£€æµ‹{content_type}æ€»é¡µæ•°å¤±è´¥: {e}")
                    return 0, 0

        return 0, 0

    def get_soup_form_url(self, url: str) -> tuple[BeautifulSoup | None, str | None]:
        """
        è·å–é¡µé¢å†…å®¹ï¼ˆå†…ç½®IPè½®æ¢é‡è¯•æœºåˆ¶ï¼‰
        Returns:
            (soup, proxy_url_str)
        """
        # Session should be created FRESH for each attempt if we want to simulate a completely new user
        # However, if we want to reuse connections for the same proxy, it should be outside.
        # Given the anti-crawl context (redirecting to other sites), a fresh session per attempt is safer
        # to avoid carrying over any "flagged" cookies.

        # session = requests.Session() <--- MOVED INSIDE LOOP

        current_headers = {
            "User-Agent": self.ua_manager.get_user_agent(),
            "Referer": f"https://guba.eastmoney.com/list,{self.secCode}.html",
            "Connection": "keep-alive",
        }

        # å†…ç½®é‡è¯•å¾ªç¯ï¼šå°è¯•3ä¸ªä¸åŒçš„ä»£ç†
        for attempt in range(1, 4):
            # Create a fresh session for each attempt to avoid cookie tracking across proxies
            session = requests.Session()

            proxies_to_use = None
            proxy_url_str = "Direct"

            if self.proxy_pool:
                proxies_to_use = self.proxy_pool.get_random_proxy()
                if not proxies_to_use:
                    # ä»£ç†æ± ç©ºäº†ï¼ŒæŠ›é”™
                    # Close session before raising
                    session.close()
                    raise NetworkException("ä»£ç†æ± è€—å°½")
                proxy_url_str = proxies_to_use.get("http", "Unknown")

            elif self.proxies:
                proxies_to_use = (
                    self.backup_proxies if self.use_backup_proxy else self.proxies
                )
                proxy_url_str = "ConfigProxy"

            try:
                # self.logger.info(f"ä¸‹è½½å°è¯• {attempt}/3: {url} (IP: {proxy_url_str})")
                response = session.get(
                    url,
                    headers=current_headers,
                    timeout=10,  # ç¨å¾®æ”¾å®½è¶…æ—¶åˆ°10ç§’
                    proxies=proxies_to_use,
                )

                if response.status_code != 200:
                    self.logger.warning(
                        f"è¯·æ±‚å¤±è´¥ [{response.status_code}] (IP: {proxy_url_str})"
                    )
                    if self.proxy_pool and proxies_to_use:
                        self.proxy_pool.remove_proxy(proxies_to_use)
                    session.close()  # Close failed session
                    continue

                html = response.content.decode("utf-8", "ignore")

                # ç®€å•å†…å®¹æ ¡éªŒ
                if "listitem" not in html:
                    if "éªŒè¯" in html or "captcha" in html:
                        self.logger.warning(f"è§¦å‘åçˆ¬è™«éªŒè¯ (IP: {proxy_url_str})")
                        if self.proxy_pool:
                            self.proxy_pool.remove_proxy(proxies_to_use)
                        session.close()
                        continue

                    # æ­£å¸¸è¿”å›200ä½†æ— å†…å®¹ï¼Œå¯èƒ½æ˜¯æœ€åä¸€é¡µï¼Œä¹Ÿå¯èƒ½æ˜¯ä»£ç†è¢«è½¯å°é”
                    # ç­–ç•¥ï¼šä¸æ‰£åˆ†ï¼Œä¸ç§»é™¤ä»£ç†ï¼Œä½†ç®—ä½œä¸€æ¬¡å¤±è´¥ï¼ˆcontinueå°è¯•å…¶ä»–ä»£ç†ï¼‰
                    # User Modified: "å¦‚æœä¸ºç©ºä¸åº”è¯¥ç»™ipæ‰£åˆ†ï¼ˆç›®æ ‡ç½‘ç«™çš„åçˆ¬åªæœ‰ä¸¤ä¸ªç­–ç•¥ï¼šé200é”™è¯¯ï¼Œæˆ–è€…å¯¼èˆªåˆ°å…¶ä»–çš„ç½‘ç«™ï¼‰"
                    # So we just continue to try another proxy if this one returned weird empty data,
                    # but we do NOT remove it from the pool.
                    self.logger.info(
                        f"é¡µé¢æ— å†…å®¹ (IP: {proxy_url_str})ï¼Œé‡è¯•ä¸‹ä¸€ä¸ªä»£ç†"
                    )
                    # Close session and continue
                    session.close()
                    continue

                session.close()
                return BeautifulSoup(html, features="lxml"), proxy_url_str

            except Exception as e:
                # å‘ç”Ÿç½‘ç»œé”™è¯¯
                self.logger.warning(f"ç½‘ç»œå¼‚å¸¸: {e} (IP: {proxy_url_str})")
                if self.proxy_pool and proxies_to_use:
                    self.proxy_pool.remove_proxy(proxies_to_use)
                session.close()
                continue

        # session.close() # Removed because session is now local in loop and closed there
        return None, None

    # --- get_data is below ---

    @retry(
        retry=(
            retry_if_exception_type(NetworkException)
            | retry_if_exception_type(ServerException)
            | retry_if_exception_type(ContentChangedException)
        ),
        stop=stop_after_attempt(5),
        wait=wait_exponential(multiplier=1, min=2, max=4),
    )
    def get_data(self, page, content_type="news", expected_total_count=None):
        """
        è·å–æŒ‡å®šé¡µé¢çš„æ•°æ® - ç»Ÿä¸€JSONè§£æç‰ˆ
        æ”¯æŒ news, report, notice ä¸‰ç§ç±»å‹
        """
        import re
        import json
        from datetime import datetime, timedelta, timezone

        # åŒ—äº¬æ—¶é—´å·¥å…·
        def get_beijing_now():
            tz_beijing = timezone(timedelta(hours=8))
            return datetime.now(tz_beijing)

        # ç±»å‹æ˜ å°„
        type_map = {"news": "1,f", "report": "2,f", "notice": "3,f"}

        # æ„é€ URL
        if page == 1:
            Url = f"https://guba.eastmoney.com/list,{self.secCode},{type_map[content_type]}.html"
        else:
            type_prefix = type_map[content_type]
            Url = f"https://guba.eastmoney.com/list,{self.secCode},{type_prefix}_{page}.html"

        try:
            self.logger.info(f"å¼€å§‹å¤„ç†é¡µé¢ {Url}")

            soup, _ = self.get_soup_form_url(Url)
            if soup is None:
                raise NetworkException(f"æ— æ³•è·å–é¡µé¢ {page} (soup is None)")

            # 1. æå– JSON æ•°æ®
            # æŸ¥æ‰¾åŒ…å« var article_list çš„è„šæœ¬
            scripts = soup.find_all("script")
            article_list_data = None

            for script in scripts:
                script_text = script.string
                if script_text and "var article_list" in script_text:
                    try:
                        # ä½¿ç”¨ raw_decode è§£å†³ "Extra data" é—®é¢˜
                        # åªéœ€è¦æ‰¾åˆ°å¼€å§‹çš„ '{'
                        start_index = script_text.find("{")
                        if start_index != -1:
                            decoder = json.JSONDecoder()
                            # raw_decode ä¼šä»å­—ç¬¦ä¸²å¼€å¤´è§£æä¸€ä¸ªå®Œæ•´çš„JSONå¯¹è±¡ï¼Œå¹¶è¿”å› (obj, end_index)
                            # æˆ‘ä»¬ä¼ å…¥ä» '{' å¼€å§‹çš„å­—ç¬¦ä¸²
                            article_list_data, _ = decoder.raw_decode(
                                script_text[start_index:]
                            )
                            break
                    except Exception as e:
                        # self.logger.warning(f"JSONè§£æå°è¯•å¤±è´¥: {e}")
                        # é™ä½æ—¥å¿—çº§åˆ«ï¼Œåªæœ‰Debugæ‰æ˜¾ç¤ºï¼Œé¿å…åˆ·å±
                        continue

            # 2. æ ¡éªŒæ•°æ®æœ‰æ•ˆæ€§
            if not article_list_data or "re" not in article_list_data:
                if "æ²¡æœ‰ç›¸å…³æ•°æ®" in str(soup):
                    return []

                if "éªŒè¯" in str(soup):
                    raise ContentChangedException("é¡µé¢åŒ…å«éªŒè¯ä¿¡æ¯ï¼Œéœ€è¦é‡è¯•")

                # å¦‚æœæ‰¾ä¸åˆ°æ•°æ®ï¼Œå¯èƒ½æ˜¯åçˆ¬æˆ–è€…é¡µé¢å˜æ›´
                raise NetworkException(
                    f"é¡µé¢ {page} æœªæ‰¾åˆ°æœ‰æ•ˆçš„ article_list JSONæ•°æ®"
                )

            # 3. åçˆ¬è™«æ ¡éªŒ (æ£€æŸ¥ count)
            if expected_total_count and expected_total_count > 0:
                page_count = article_list_data.get("count", 0)
                if abs(page_count - expected_total_count) > 100:
                    self.logger.warning(
                        f"âš ï¸ åçˆ¬è™«è­¦å‘Š: é¡µé¢{page} count({page_count}) ä¸é¢„æœŸ({expected_total_count}) å·®å¼‚è¿‡å¤§"
                    )
                    raise ContentChangedException(
                        f"é¡µé¢å†…å®¹ä¸åŒ¹é… (Count deviation: {page_count} vs {expected_total_count})"
                    )

            # 4. è§£ææ•°æ®åˆ—è¡¨
            items = article_list_data["re"]
            batch_data = []

            for item in items:
                try:
                    post_id = str(item.get("post_id"))
                    title = item.get("post_title")

                    if not post_id or not title:
                        continue

                    # ç»Ÿä¸€URLå¤„ç†
                    raw_url = item.get("Art_Url")
                    if raw_url:
                        url = raw_url
                    else:
                        url = f"https://guba.eastmoney.com/news,{self.secCode},{post_id}.html"

                    # ç»Ÿä¸€æ„å»ºæ•°æ®å­—å…¸
                    data_json = {
                        "stock_code": self.secCode,
                        "content_type": content_type,
                        "title": title,
                        "url": url,
                        "url_id": post_id,
                        "read_count": item.get("post_click_count", 0),
                        "comment_count": item.get("post_comment_count", 0),
                        "publish_time": item.get("post_publish_time"),  # å®Œæ•´æ—¶é—´
                        # Author / Nickname
                        "author": item.get("user_nickname"),
                        # Report Specific
                        "grade": item.get("grade_type"),  # è¯„çº§
                        "institution": item.get("institution"),  # æœºæ„
                        # Notice Specific
                        "notice_type": item.get("notice_type"),  # å…¬å‘Šç±»å‹
                        "summary": title,
                        "source": "official",
                        "created_at": get_beijing_now(),
                        "updated_at": get_beijing_now(),
                    }

                    batch_data.append(data_json)
                except Exception as e:
                    self.logger.warning(f"è§£ææ¡ç›®å¤±è´¥: {e}")
                    continue

            self.logger.info(f"æˆåŠŸè·å–é¡µé¢ {page}: {len(batch_data)} æ¡æ•°æ®")
            return batch_data

        except ContentChangedException as e:
            raise e  # ç›´æ¥æŠ›å‡ºï¼Œè§¦å‘retry
        except Exception as e:
            time.sleep(random.uniform(0.5, 3))
            self.logger.error(f"è·å–é¡µé¢æ•°æ®å¤±è´¥: {e}, ç›´æ¥è·³è¿‡è¯¥é¡µé¢ï¼")
            # raise NetworkException(e)

    # æ£€æŸ¥ç´¢å¼•å­˜åœ¨æ€§
    def index_exists_by_name(self, collection, index_name):
        return index_name in collection.index_information()

    def _crawl_single_page(
        self, page: int, content_type: str, total_count: int
    ) -> list:
        """
        çº¿ç¨‹æ± ä½¿ç”¨çš„å•é¡µçˆ¬å–åŒ…è£…æ–¹æ³•
        å¢åŠ é€»è¾‘ï¼šé’ˆå¯¹æœ€åä¸€é¡µï¼ˆæˆ–æ¥è¿‘æœ«å°¾é¡µï¼‰å¯èƒ½ä¸ºç©ºçš„æƒ…å†µè¿›è¡Œç‰¹æ®Šå¤„ç†
        è¿”å› None è¡¨ç¤ºå¤±è´¥ï¼Œè¿”å› [] è¡¨ç¤ºæˆåŠŸä½†æ— æ•°æ®
        """
        retry_count = 0
        max_retries = 3  # é»˜è®¤ç½‘ç»œé‡è¯•
        empty_retries = 0  # ç©ºæ•°æ®é‡è¯•

        # åˆ¤æ–­æ˜¯å¦å¯èƒ½æ˜¯æœ€åä¸€é¡µ (æ¯é¡µ80æ¡)
        is_last_page = False
        if total_count > 0:
            if page * 80 >= total_count:
                is_last_page = True

        while retry_count < max_retries:
            try:
                # åªè´Ÿè´£ä¸‹è½½
                data_list = self.get_data(
                    page, content_type, expected_total_count=total_count
                )

                # [æ–°å¢] é’ˆå¯¹æœ€åä¸€é¡µä¸ºç©ºçš„BUGå¤„ç† (æ•°æ®ä¸ºç©ºçš„æƒ…å†µ)
                if not data_list and is_last_page:
                    if empty_retries < 2:
                        empty_retries += 1
                        time.sleep(1)
                        self.logger.info(
                            f"é¡µé¢{page} (æœ€åä¸€é¡µ?) è¿”å›æ•°æ®ç©ºï¼Œé‡è¯•ç¬¬{empty_retries}æ¬¡..."
                        )
                        continue  # è§¦å‘é‡è¯•
                    else:
                        self.logger.info(
                            f"é¡µé¢{page} (æœ€åä¸€é¡µ) è¿ç»­ä¸ºç©ºï¼Œè§†ä¸ºæ­£å¸¸ç»“æŸ (ç½‘ç«™BUG)"
                        )
                        return []  # explicitly return empty list for success

                return data_list

            except NetworkException as e:
                # [æ–°å¢] ä¸“é—¨å¤„ç† "soup is None" è¿™ç§æƒ…å†µ
                if "soup is None" in str(e):
                    # å¦‚æœæ˜¯æœ€åä¸€é¡µï¼Œä¿ç•™åŸæœ‰çš„å®¹é”™é€»è¾‘
                    if is_last_page:
                        if empty_retries < 2:
                            empty_retries += 1
                            time.sleep(1)
                            self.logger.info(
                                f"é¡µé¢{page} (æœ€åä¸€é¡µ?) è·å–ä¸ºç©º(soup=None)ï¼Œé‡è¯•ç¬¬{empty_retries}æ¬¡..."
                            )
                            continue
                        else:
                            self.logger.info(
                                f"é¡µé¢{page} (æœ€åä¸€é¡µ) è¿ç»­è·å–ä¸ºç©ºï¼Œè§†ä¸ºæ­£å¸¸ç»“æŸ"
                            )
                            return []

                    # å¯¹äºéæœ€åä¸€é¡µï¼Œå¦‚æœsoup is Noneï¼Œè¯´æ˜åº•å±‚å·²ç»é‡è¯•å¤šæ¬¡å‡å¤±è´¥
                    # æ­¤æ—¶ç›´æ¥æ”¾å¼ƒè¯¥é¡µï¼Œä¸å†è¿›è¡Œå¤–å±‚é‡è¯•
                    self.logger.warning(
                        f"é¡µé¢{page} æ— æ³•è·å–æ•°æ® (soup is None)ï¼Œæ”¾å¼ƒè¯¥é¡µä»»åŠ¡"
                    )
                    return []

                # å…¶ä»–ç½‘ç»œé”™è¯¯ï¼Œèµ°æ­£å¸¸é‡è¯•
                retry_count += 1
                if retry_count >= max_retries:
                    self.logger.error(f"é¡µé¢{page}é‡è¯•{max_retries}æ¬¡åä»å¤±è´¥: {e}")
                    return None  # Failure
                time.sleep(random.uniform(1, 2))

            except Exception as e:
                retry_count += 1
                if retry_count >= max_retries:
                    self.logger.error(f"é¡µé¢{page}é‡è¯•{max_retries}æ¬¡åä»å¤±è´¥: {e}")
                    return None  # Failure
                time.sleep(random.uniform(1, 2))

        return None  # Failure if loop finishes without return

    def main(self):
        # åˆ›å»ºå¤åˆå”¯ä¸€ç´¢å¼•ï¼šstock_code + content_type + url_id
        try:
            if not self.index_exists_by_name(self.col, "idx_stock_type_url"):
                self.logger.info("åˆ›å»ºå¤åˆå”¯ä¸€ç´¢å¼•: stock_code + content_type + url_id")
                self.col.create_index(
                    [("stock_code", 1), ("content_type", 1), ("url_id", 1)],
                    unique=True,
                    name="idx_stock_type_url",
                )

            # åˆ›å»ºæŸ¥è¯¢ä¼˜åŒ–ç´¢å¼•
            if not self.index_exists_by_name(self.col, "idx_stock_type_time"):
                self.logger.info("åˆ›å»ºæŸ¥è¯¢ç´¢å¼•: stock_code + content_type + crawl_time")
                self.col.create_index(
                    [("stock_code", 1), ("content_type", 1), ("crawl_time", -1)],
                    name="idx_stock_type_time",
                )
        except Exception as e:
            self.logger.error(f"åˆ›å»ºç´¢å¼•å¤±è´¥: {e}")
            pass

        # ä¸‰ç§å†…å®¹ç±»å‹
        content_types = ["news", "report", "notice"]
        type_names = {"news": "èµ„è®¯", "report": "ç ”æŠ¥", "notice": "å…¬å‘Š"}

        for content_type in content_types:
            # è‡ªåŠ¨æ£€æµ‹æ€»é¡µæ•°å’Œæ€»æ¡æ•°
            total_pages, total_count = self.get_total_pages(content_type)

            if total_pages == 0:
                print(f"âš ï¸ æ— æ³•è·å–{type_names[content_type]}é¡µæ•°ï¼Œè·³è¿‡")
                continue

            print(f"\n{'=' * 60}")
            print(f"å¼€å§‹çˆ¬å–{type_names[content_type]}ï¼Œå…±{total_pages}é¡µ")
            print(f"{'=' * 60}")

            # é‡ç½®é‡å¤è®¡æ•°å™¨å’Œå¹´ä»½æ¨æ–­çŠ¶æ€
            self.consecutive_duplicate_pages = 0
            self.last_month = None
            self.current_year = datetime.now().year

            # çˆ¬å–æ‰€æœ‰é¡µé¢ - å¤šçº¿ç¨‹ç‰ˆæœ¬
            # è¿™é‡Œçš„æ”¹å˜æ˜¯ï¼šå¤šçº¿ç¨‹ä¸‹è½½ï¼Œä½†å•çº¿ç¨‹é¡ºåºå¤„ç†ç»“æœï¼ˆå…¥åº“ï¼‰

            # [æ–°å¢] è®°å½•IPæ± å¥åº·çŠ¶å†µ
            if self.proxy_pool:
                pool_size = self.proxy_pool.count()
                self.logger.info(f"å½“å‰IPæ± å¥åº·çŠ¶å†µ: {pool_size}ä¸ªå¯ç”¨ä»£ç†")
                if pool_size < 5:
                    self.logger.warning(
                        f"âš ï¸ IPæ± æ•°é‡è¾ƒä½ ({pool_size})ï¼Œçˆ¬å–é€Ÿåº¦å¯èƒ½å—é™"
                    )

            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                # æäº¤æ‰€æœ‰é¡µé¢ä»»åŠ¡
                # page -> future
                future_map = {
                    page: executor.submit(
                        self._crawl_single_page, page, content_type, total_count
                    )
                    for page in range(1, total_pages + 1)
                }

                # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
                # [Request] Add stock code to tqdm description for err.log clarity
                with tqdm(
                    total=total_pages,
                    desc=f"[{self.secCode}] {type_names[content_type]}",
                ) as pbar:
                    should_stop = False

                    # ä¸¥æ ¼æŒ‰é¡ºåº 1, 2, 3... è·å–ç»“æœ
                    for page in range(1, total_pages + 1):
                        future = future_map[page]
                        try:
                            # è·å–ç»“æœï¼ˆå¦‚æœè¯¥é¡µè¿˜æ²¡ä¸‹è½½å®Œï¼Œä¼šé˜»å¡ç›´åˆ°ä¸‹è½½å®Œï¼‰
                            batch_data = future.result()

                            # [æ–°å¢] å¦‚æœè¿”å›Noneï¼Œè¯´æ˜è¯¥é¡µçˆ¬å–å¤±è´¥ï¼ˆé‡è¯•è€—å°½ï¼‰ï¼Œè·³è¿‡å¤„ç†ï¼Œä¸è®¡å…¥é‡å¤åœæ­¢é€»è¾‘
                            if batch_data is None:
                                self.logger.error(f"é¡µé¢ {page} æœ€ç»ˆå¤±è´¥ï¼Œè·³è¿‡å¤„ç†")
                                pbar.update(1)
                                continue

                            # 1. é¡ºåºå¤„ç† (JSONè§£ææ¨¡å¼ä¸‹ä¸éœ€è¦æ¨æ–­å¹´ä»½)
                            valid_data = []
                            for data in batch_data:
                                # JSONç›´æ¥è¿”å›å®Œæ•´æ—¶é—´ï¼Œæ— éœ€æ¨æ–­
                                # data["publish_time"] å·²ç»æ˜¯ "2026-01-19 19:02:17" æ ¼å¼
                                valid_data.append(data)

                            # 2. å†™å…¥æ•°æ®åº“
                            new_count = 0
                            if valid_data:
                                try:
                                    result = self.col.insert_many(
                                        valid_data, ordered=False
                                    )
                                    new_count = len(result.inserted_ids)
                                except BulkWriteError as e:
                                    new_count = e.details.get("nInserted", 0)
                                except Exception as e:
                                    self.logger.error(f"å†™å…¥DBå¤±è´¥: {e}")

                            # 3. é¡µçº§é‡å¤æ£€æµ‹
                            if new_count == 0:
                                self.consecutive_duplicate_pages += 1
                                pbar.set_postfix(
                                    {
                                        "æ–°å¢": 0,
                                        "çŠ¶æ€": f"é¡µé‡å¤x{self.consecutive_duplicate_pages}",
                                    }
                                )
                            else:
                                self.consecutive_duplicate_pages = 0
                                pbar.set_postfix({"æ–°å¢": new_count, "çŠ¶æ€": "è¿›è¡Œä¸­"})

                            # 4. é˜ˆå€¼æ£€æŸ¥
                            if (
                                self.consecutive_duplicate_pages
                                >= self.duplicate_page_threshold
                            ):
                                self.logger.warning(
                                    f"â¹ è¿ç»­{self.consecutive_duplicate_pages}é¡µé‡å¤ï¼Œè·³è¿‡å‰©ä½™"
                                )
                                should_stop = True

                            pbar.update(1)

                            if should_stop:
                                # å–æ¶ˆå‰©ä½™ä»»åŠ¡
                                for p in range(page + 1, total_pages + 1):
                                    if p in future_map:
                                        future_map[p].cancel()
                                break

                        except Exception as e:
                            self.logger.error(f"å¤„ç†é¡µé¢{page}æµç¨‹å¤±è´¥: {e}")
                            pbar.update(1)

            if should_stop:
                print(f"\nâ¹ï¸ {self.secCode} {type_names[content_type]} æå‰ç»“æŸ")
            else:
                print(f"\nâœ“ {self.secCode} {type_names[content_type]} å®Œæˆ")

            time.sleep(random.uniform(0, 1))
            self.num_start = 0


if __name__ == "__main__":
    """ç›´æ¥è¿è¡Œæ­¤æ–‡ä»¶æ—¶çš„å…¥å£"""
    import sys

    # è¯»å–é…ç½®
    config_path = "config.ini"
    config = configparser.ConfigParser()
    config.read(config_path, encoding="utf-8")

    # ä»é…ç½®æ–‡ä»¶è¯»å–å‚æ•°
    sec_code = config.get("mainClass", "secCode", fallback="600519")
    pages_start = config.getint("mainClass", "pages_start", fallback=1)
    pages_end = config.getint("mainClass", "pages_end", fallback=10)
    collection_name = config.get("mainClass", "collectionName", fallback="stock_news")
    db_name = config.get("MongoDB", "database", fallback="guba")

    print("=" * 60)
    print("ä¸œæ–¹è´¢å¯Œè‚¡å§çˆ¬è™« - å®˜æ–¹å’¨è¯¢æŠ“å– (æ–°ç‰ˆ)")
    print("=" * 60)
    print(f"è‚¡ç¥¨ä»£ç : {sec_code}")
    print(f"æŠ“å–èŒƒå›´: ç¬¬ {pages_start} é¡µåˆ°ç¬¬ {pages_end} é¡µ")
    print(f"æ•°æ®å­˜å‚¨: MongoDB - {db_name}.{collection_name}")
    print(f"æ•°æ®ç»“æ„: ç»Ÿä¸€Collection (æ‰€æœ‰è‚¡ç¥¨å…±ç”¨)")
    print("=" * 60)

    try:
        # åˆ›å»ºçˆ¬è™«å®ä¾‹
        crawler = guba_comments(
            config_path=config_path,
            config=config,
            secCode=sec_code,
            pages_start=pages_start,
            pages_end=pages_end,
            num_start=0,
            MongoDB=True,
            collectionName=collection_name,
            full_text=False,
        )

        # å¼€å§‹çˆ¬å–
        print("\nå¼€å§‹çˆ¬å–...")
        crawler.main()

        print("\n" + "=" * 60)
        print("âœ… çˆ¬å–å®Œæˆï¼")
        print("=" * 60)

    except KeyboardInterrupt:
        print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­çˆ¬å–")
        sys.exit(0)
    except Exception as e:
        print(f"\n\nâŒ çˆ¬å–å¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
        sys.exit(1)
