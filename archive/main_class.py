# -*- coding: utf-8 -*-
# @Time    : 2023/2/11 21:27
# @Author  : Euclid-Jie
# @File    : main_class.py
import pandas as pd
import requests
from bs4 import BeautifulSoup
from tqdm import tqdm
import logging
from datetime import datetime
# from retrying import retry
from typing import Optional, Union
from concurrent.futures import ThreadPoolExecutor, as_completed
# from Utils.MongoClient import MongoClient
# from Utils.EuclidDataTools import CsvClient
# from TreadCrawler import RedisClient
import configparser
from storage.database_client import DatabaseManager
from core.user_agent_manager import get_user_agent_manager
import time
import random

from tenacity import retry,retry_if_exception_type,stop_after_attempt,wait_exponential
from pymongo.errors import DuplicateKeyError, BulkWriteError
from core.proxy_manager import ProxyManager  # ä½¿ç”¨ç»Ÿä¸€çš„ä»£ç†æ± ç®¡ç†å™¨
# å®šä¹‰è‡ªå®šä¹‰å¼‚å¸¸ç±»
class NetworkException(Exception): pass
class ServerException(Exception): pass
class ContentChangedException(Exception): pass


class guba_comments:
    """
    this class is designed for get hot comments for guba, have two method which can be set at def get_data()
    1ã€all: https://guba.eastmoney.com/list,600519_1.html, secCode: 600519, page: 1
    2ã€hot: https://guba.eastmoney.com/list,600519,99_1.html secCode: 600519, page: 1

    because to the ip control, this need to set proxies pools
    by using proxies https://www.kuaidaili.com/usercenter/overview/, can solve this problem

    Program characteristics:
        1ã€default write data to mongoDB, by init "MogoDB=False", can switch to write data to csv file
        2ã€Use retry mechanism, once rise error, the program will restart at the least page and num (each page has 80 num)

    """

    failed_proxies = {}
    proxy_fail_times_treshold = 3

    def __init__(
        self,
        config_path:str,
        config: configparser.ConfigParser,
        secCode,
        pages_start: int = 0,
        pages_end: int = 100,
        num_start: int = 0,
        MongoDB: bool = True,
        collectionName : str = 'default',
        full_text: bool = False,
        max_workers: int = 8,  # æ–°å¢ï¼šå¤šçº¿ç¨‹å¹¶å‘æ•°
    ):
        self.config_path = config_path
        self.db_manager = DatabaseManager(self.config_path)
        
        # å‚æ•°åˆå§‹åŒ– - ä¿®å¤secCodeæœªèµ‹å€¼çš„bug
        if isinstance(secCode, int):
            # è¡¥é½6ä½æ•°
            self.secCode = str(secCode).zfill(6)
        elif isinstance(secCode, str):
            self.secCode = secCode
        else:
            self.secCode = str(secCode)
        
        self.pages_start = pages_start
        self.pages_end = pages_end
        self.num_start = num_start
        self.full_text = full_text
        self._year = pd.Timestamp.now().year
        
        # å¹´ä»½æ¨æ–­ï¼šç”¨äºå¤„ç†publish_timeæ²¡æœ‰å¹´ä»½çš„é—®é¢˜
        self.current_year = datetime.now().year
        self.last_month = None  # è®°å½•ä¸Šä¸€æ¡æ–°é—»çš„æœˆä»½ï¼Œç”¨äºæ¨æ–­å¹´ä»½
        
        # å¢é‡æ›´æ–°ä¼˜åŒ–ï¼šè¿ç»­é‡å¤è®¡æ•°å™¨
        self.consecutive_duplicates = 0
        self.duplicate_threshold = 5  # è¿ç»­5æ¡é‡å¤åˆ™åœæ­¢çˆ¬å–
        
        # å¤šçº¿ç¨‹é…ç½®
        self.max_workers = max_workers

        # redis client for full_text_Crawler
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨Redis
        self.redis_enabled = config.getboolean('Redis', 'enabled', fallback=True)
        if self.redis_enabled:
            self.redis_client = self.db_manager.get_redis_client()
        else:
            self.redis_client = None

        # é…ç½®æ–‡ä»¶å¯ä»¥è¦†ç›–å‚æ•°
        if config.has_option("mainClass", "secCode"):
            self.secCode = config.get("mainClass", "secCode")
            print(
                f"secCode has been overridden by {self.secCode} in the configuration file."
            )
        if config.has_option("mainClass", "pages_start"):
            self.pages_start = int(config.get("mainClass", "pages_start"))
            print(
                f"pages_start has been overridden by {self.pages_start} in the configuration file."
            )
        if config.has_option("mainClass", "pages_end"):
            self.pages_end = int(config.get("mainClass", "pages_end"))
            print(
                f"pages_end has been overridden by {self.pages_end} in the configuration file."
            )
        if config.has_option("mainClass", "collectionname"):
            collectionName = config.get("mainClass", "collectionname")
            print(
                f"collectionName has been overridden by {collectionName} in the configuration file."
            )
        if config.has_option("mainClass", "max_workers"):
            self.max_workers = int(config.get("mainClass", "max_workers"))
            print(
                f"max_workers has been overridden by {self.max_workers} in the configuration file."
            )

        # choose one save method, default MongoDB
        # 1ã€csv
        # 2ã€MongoDB
        # å¸–å­æ•°æ®å­˜å‚¨åˆ°é…ç½®çš„æ•°æ®åº“
        # self.log_path = config.get("logging", "log_file")
        
        # print("è¿™æ˜¯è¯»å–åˆ°æ ‡é¢˜çˆ¬å–çš„log"+self.log_path)
        
        # ä»é…ç½®æ–‡ä»¶è¯»å–æ•°æ®åº“åç§°å’ŒCollectionåç§°
        db_name = config.get('MongoDB', 'database', fallback='guba')
        # ä½¿ç”¨ç»Ÿä¸€çš„Collectionåç§°ï¼šstock_newsï¼ˆæ‰€æœ‰è‚¡ç¥¨å…±ç”¨ï¼‰
        collection_name = config.get('mainClass', 'collectionName', fallback='stock_news')
        # ä½¿ç”¨ database_client çš„ MongoDB å®¢æˆ·ç«¯
        self.col = self.db_manager.get_mongo_client(db_name, collection_name).collection
        self.collection_name = collection_name  # ä¿å­˜Collectionåç§°ä¾›åç»­ä½¿ç”¨

        # log setting
        # log_format = "%(levelname)s %(asctime)s %(filename)s %(lineno)d %(message)s"
        # logging.basicConfig(filename=self.log_path, format=log_format, level=logging.INFO,encoding = 'utf-8')
        
        # è®¾ç½®æ—¥å¿—çº§åˆ«ä¸ºWARNINGï¼Œå‡å°‘åˆ·å±ï¼ˆåªæ˜¾ç¤ºè­¦å‘Šå’Œé”™è¯¯ï¼‰
        self.logger = logging.getLogger(config.get('logging', 'log_file', fallback='main_controller.log'))
        if not self.logger.handlers:  # é¿å…é‡å¤æ·»åŠ handler
            self.logger.setLevel(logging.WARNING)  # åªæ˜¾ç¤ºWARNINGåŠä»¥ä¸Šçº§åˆ«
            handler = logging.StreamHandler()
            handler.setLevel(logging.WARNING)
            self.logger.addHandler(handler)
        
        # User-Agentç®¡ç†å™¨åˆå§‹åŒ–
        self.ua_manager = get_user_agent_manager('random')  # ä½¿ç”¨éšæœºæ¨¡å¼
        
        # header setting - ä½¿ç”¨åŠ¨æ€User-Agent
        self.header = {
            "User-Agent": self.ua_manager.get_user_agent(),
        }
        
        # # proxies setting
        # if config.has_option("proxies", "tunnel"):
        #     tunnel = config.get("proxies", "tunnel")
        #     self.proxies = {
        #         "http": "http://%(proxy)s/" % {"proxy": tunnel},
        #         "https": "http://%(proxy)s/" % {"proxy": tunnel},
        #     }
        # else:
        #     self.proxies = None
        
        # éš§é“åŸŸå:ç«¯å£å·
        tunnel = "x291.kdltps.com:15818"

        # ç”¨æˆ·åå¯†ç æ–¹å¼
        username = "t15462021520395"
        password = "wkjzgkdb"
        
        # ä»£ç†è®¾ç½®
        proxy_enabled = config.getboolean('proxies', 'enabled', fallback=False)
        use_free_proxy_pool = config.getboolean('proxies', 'use_free_proxy_pool', fallback=False)
        
        # åˆå§‹åŒ–ä»£ç†æ± 
        self.proxy_pool = None
        if use_free_proxy_pool:
            print("\nğŸ”§ åˆå§‹åŒ–å…è´¹ä»£ç†æ± ...")
            # ä»é…ç½®è¯»å–ä»£ç†æ± é˜ˆå€¼
            min_proxy_threshold = config.getint('proxies', 'min_proxy_count', fallback=5)
            self.proxy_pool = ProxyPool(
                target_url="https://guba.eastmoney.com/",
                min_threshold=min_proxy_threshold
            )
            # å°è¯•ä»æ–‡ä»¶åŠ è½½
            if not self.proxy_pool.load_from_file():
                # æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå»ºç«‹æ–°æ± ï¼ˆæ¯ä¸ªæºæœ€å¤š200ä¸ªï¼Œå…¨é¢æµ‹è¯•ï¼Œä¸é™æ—¶é—´ï¼‰
                print("â³ å¼€å§‹å…¨é¢ä»£ç†æµ‹è¯•ï¼Œé¢„è®¡5-10åˆ†é’Ÿ...")
                self.proxy_pool.build_pool(max_workers=50, max_per_source=200)
                self.proxy_pool.save_to_file()
            print("âœ… ä»£ç†æ± å°±ç»ª\n")
        
        if proxy_enabled and not use_free_proxy_pool:
            # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å›ºå®šä»£ç†
            username = config.get('proxies', 'username', fallback='t15462021520395')
            password = config.get('proxies', 'password', fallback='wkjzgkdb')
            tunnel = config.get('proxies', 'tunnel', fallback='')
            
            self.proxies = {
                "http": f"http://{username}:{password}@{tunnel}/",
                "https": f"http://{username}:{password}@{tunnel}/"
            }
            self.backup_proxies = {
                "http": f"http://{username}:{password}@x292.kdltps.com:15818/",
                "https": f"http://{username}:{password}@x292.kdltps.com:15818/"
            }
            print("âœ“ ä»£ç†å·²å¯ç”¨")
        elif use_free_proxy_pool:
            # ä½¿ç”¨å…è´¹ä»£ç†æ± 
            self.proxies = None  # åŠ¨æ€è·å–
            self.backup_proxies = None
            print("âœ“ å…è´¹ä»£ç†æ± å·²å¯ç”¨")
        else:
            self.proxies = None
            self.backup_proxies = None
            print("âš ï¸ ä»£ç†å·²ç¦ç”¨ - ç›´æ¥è¿æ¥")
        
        self.use_backup_proxy = False

    def _change_proxy_ip(self):
        """è°ƒç”¨å¿«ä»£ç†æ›´æ¢éš§é“IPæ¥å£"""
        try:
            import requests
            change_url = "https://tps.kdlapi.com/api/changetpsip"
            params = {
                "secret_id": "oqifdb1h1ykoxm8comcv",
                "signature": "vhp8ervln42dkh85ht3ijw6adctj1wah"
            }
            
            response = requests.get(change_url, params=params, timeout=3)
            if response.status_code == 200:
                self.logger.info("éš§é“IPæ›´æ¢æˆåŠŸ")
                return True
            else:
                self.logger.error(f"æ›´æ¢éš§é“IPå¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return False
        except Exception as e:
            self.logger.error(f"æ›´æ¢éš§é“IPæ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
            return False

    @staticmethod
    def clear_str(str_raw):
        for pat in ["\n", " ", " ", "\r", "\xa0", "\n\r\n"]:
            str_raw.strip(pat).replace(pat, "")
        return str_raw

    @staticmethod
    def run_thread_pool_sub(target, args, max_work_count):
        with ThreadPoolExecutor(max_workers=max_work_count) as t:
            res = [t.submit(target, i) for i in args]
            return res
    
    def get_total_pages(self, content_type='news'):
        """
        è‡ªåŠ¨æ£€æµ‹æŒ‡å®šå†…å®¹ç±»å‹çš„æ€»é¡µæ•°
        
        é€šè¿‡è§£æé¡µé¢ä¸­çš„JavaScriptå˜é‡ var article_list = {...} 
        æå– count å­—æ®µï¼ˆæ€»å¸–å­æ•°ï¼‰ï¼Œç„¶åè®¡ç®—æ€»é¡µæ•° = ceil(count / 80)
        
        Args:
            content_type: 'news' (èµ„è®¯) | 'report' (ç ”æŠ¥) | 'notice' (å…¬å‘Š)
        
        Returns:
            int: æ€»é¡µæ•°ï¼Œå¦‚æœæ£€æµ‹å¤±è´¥è¿”å›0
        """
        import re
        import math
        
        # ç±»å‹æ˜ å°„
        type_map = {
            'news': '1,f',
            'report': '2,f',
            'notice': '3,f'
        }
        
        if content_type not in type_map:
            self.logger.error(f"æœªçŸ¥çš„å†…å®¹ç±»å‹: {content_type}")
            return 0
        
        # æ„é€ ç¬¬ä¸€é¡µURL
        url = f"https://guba.eastmoney.com/list,{self.secCode},{type_map[content_type]}.html"
        
        try:
            soup = self.get_soup_form_url(url)
            if not soup:
                return 0
            
            # æŸ¥æ‰¾æ‰€æœ‰scriptæ ‡ç­¾
            scripts = soup.find_all('script')
            
            for script in scripts:
                script_text = script.string
                if script_text and 'var article_list' in script_text:
                    # æ‰¾åˆ°åŒ…å« article_list çš„è„šæœ¬
                    # ä½¿ç”¨æ­£åˆ™æå– "count": æ•°å­—
                    match = re.search(r'"count"\s*:\s*(\d+)', script_text)
                    if match:
                        total_count = int(match.group(1))
                        
                        # å®‰å…¨æ£€æŸ¥ï¼šå¦‚æœcountå¼‚å¸¸å¤§ï¼Œå¯èƒ½æ˜¯æœåŠ¡å™¨è¿”å›äº†é”™è¯¯æ•°æ®
                        if total_count > 50000:
                            print(f"âš ï¸ {content_type}: countå€¼å¼‚å¸¸å¤§({total_count})ï¼Œå¯èƒ½è¢«åçˆ¬è™«ï¼Œä½¿ç”¨ä¿å®ˆå€¼")
                            return 1  # è¿”å›1é¡µï¼Œè®©ç”¨æˆ·å¯Ÿè§‰é—®é¢˜
                        
                        # æ¯é¡µ80æ¡ï¼Œè®¡ç®—æ€»é¡µæ•°
                        total_pages = math.ceil(total_count / 80)
                        print(f"âœ“ {content_type}: å…±{total_count}æ¡æ•°æ®ï¼Œ{total_pages}é¡µ")
                        return total_pages
            
            # å¦‚æœæ²¡æ‰¾åˆ°JavaScriptå˜é‡ï¼Œå›é€€åˆ°åªè¿”å›1é¡µ
            print(f"âš ï¸ {content_type}: æœªæ‰¾åˆ°article_listå˜é‡ï¼Œé»˜è®¤1é¡µ")
            return 1
            
        except Exception as e:
            print(f"âŒ {content_type}: æ£€æµ‹é¡µæ•°å¼‚å¸¸ - {e}")
            self.logger.error(f"æ£€æµ‹{content_type}æ€»é¡µæ•°å¤±è´¥: {e}")
            return 0
    
    def _infer_year_for_publish_time(self, publish_time_raw):
        """
        æ¨æ–­å‘å¸ƒæ—¶é—´çš„å¹´ä»½
        
        é€»è¾‘ï¼š
        1. è§£æå‡ºæœˆä»½
        2. å¦‚æœå½“å‰æœˆä»½ > ä¸Šä¸€æ¡çš„æœˆä»½+3ï¼ˆä¾‹å¦‚ä»1æœˆåˆ°12æœˆï¼‰ï¼Œè¯´æ˜è·¨å¹´äº†ï¼Œå¹´ä»½-1
        3. è¿”å›å®Œæ•´çš„å¹´æœˆæ—¥æ—¶é—´å­—ç¬¦ä¸²
        
        Args:
            publish_time_raw: åŸå§‹æ—¶é—´å­—ç¬¦ä¸²ï¼Œä¾‹å¦‚ "01-21 15:30" æˆ– "12-31 23:59"
        
        Returns:
            å®Œæ•´æ—¶é—´å­—ç¬¦ä¸²ï¼Œä¾‹å¦‚ "2026-01-21 15:30" æˆ– "2025-12-31 23:59"
        """
        try:
            # è§£ææœˆä»½ï¼ˆä¾‹å¦‚ï¼š"01-21 15:30" -> 1ï¼‰
            parts = publish_time_raw.split()
            if len(parts) < 1:
                return publish_time_raw
            
            date_part = parts[0]  # "01-21"
            month = int(date_part.split("-")[0])
            
            # æ¨æ–­å¹´ä»½é€»è¾‘
            if self.last_month is not None:
                # å¦‚æœå½“å‰æœˆä»½ > ä¸Šä¸€æ¡æœˆä»½+3ï¼Œè¯´æ˜è·¨å¹´åå‘äº†ï¼ˆä¾‹å¦‚ï¼š1æœˆ -> 12æœˆï¼‰
                if month > self.last_month + 3:
                    self.current_year -= 1
            
            self.last_month = month
            
            # æ‹¼æ¥å®Œæ•´æ—¶é—´
            return f"{self.current_year}-{publish_time_raw}"
            
        except Exception as e:
            # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›åŸå§‹å­—ç¬¦ä¸²
            return publish_time_raw

    # @retry(stop_max_attempt_number=5, wait_fixed=2000)  # æœ€å¤šå°è¯•5æ¬¡ï¼Œæ¯æ¬¡é—´éš”2ç§’
    def get_soup_form_url(self, url: str) -> BeautifulSoup:
        session = requests.Session()
        current_headers = {
            "User-Agent": self.ua_manager.get_user_agent(),
            "Referer": f"https://guba.eastmoney.com/list,{self.secCode}.html",
            "Connection": "keep-alive",
        }
        
        proxies_to_use = None
        if self.proxy_pool:
            proxies_to_use = self.proxy_pool.get_random_proxy()
            if not proxies_to_use:
                # å…³é”®ï¼šå¦‚æœå¯ç”¨äº†æ± ä½†æ‹¿ä¸åˆ°ä»£ç†ï¼ŒæŠ›é”™é˜²æ­¢æ³„éœ²æœ¬åœ°IP
                raise NetworkException("ä»£ç†æ± å·²ç©ºï¼Œä¸ºä¿æŠ¤æœ¬åœ°IPï¼Œåœæ­¢è¯·æ±‚")
        elif self.proxies:
            proxies_to_use = self.backup_proxies if self.use_backup_proxy else self.proxies

        try:
            response = session.get(
                url,
                headers=current_headers,
                timeout=5, # ç¼©çŸ­è¶…æ—¶åˆ°5ç§’
                proxies=proxies_to_use
            )
            session.close()

            if response.status_code != 200:
                # å¦‚æœæ˜¯403æˆ–502ï¼Œè¯´æ˜ä»£ç†è¢«å°æˆ–å¤±æ•ˆ
                if self.proxy_pool and proxies_to_use:
                    self.proxy_pool.remove_proxy(proxies_to_use)
                return None

            html = response.content.decode("utf-8", "ignore")
            if "listitem" not in html:
                if "éªŒè¯" in html or "captcha" in html:
                    if self.proxy_pool: self.proxy_pool.remove_proxy(proxies_to_use)
                return None
            return BeautifulSoup(html, features="lxml")

        except Exception as e:
            # å‘ç”Ÿä»»ä½•ç½‘ç»œé”™è¯¯ï¼Œç«‹å³ç§»é™¤è¯¥ä»£ç†
            if self.proxy_pool and proxies_to_use:
                self.proxy_pool.remove_proxy(proxies_to_use)
            self.logger.warning(f"ä»£ç†è¯·æ±‚å¼‚å¸¸: {e}")
            return None

    def get_data_json(self, item, content_type='news'):
        """
        è§£ææ•°æ®é¡¹ï¼Œæ”¯æŒä¸‰ç§å†…å®¹ç±»å‹
        
        Args:
            item: BeautifulSoupå…ƒç´ 
            content_type: 'news' (èµ„è®¯) | 'report' (ç ”æŠ¥) | 'notice' (å…¬å‘Š)
        
        Returns:
            dict: æ•°æ®å­—å…¸ï¼Œè§£æå¤±è´¥è¿”å›None
        """

        tds = item.find_all("td")
        if len(tds) < 5:
            # é™é»˜è·³è¿‡ï¼Œä¸æ‰“å°è­¦å‘Š
            return None
        
        try:
            # æå–URLå’ŒID
            href = tds[2].a["href"]
            full_url = "https://guba.eastmoney.com" + href
            
            # ä»URLä¸­æå–å”¯ä¸€IDï¼ˆä¾‹å¦‚ï¼š/news,600519,1234567890.html -> 1234567890ï¼‰
            try:
                url_id = href.split(",")[-1].replace(".html", "").strip()
            except:
                url_id = href  # å¦‚æœæå–å¤±è´¥ï¼Œä½¿ç”¨å®Œæ•´href
            
            # æ•°å­—å­—æ®µè½¬æ¢
            try:
                read_count = int(tds[0].text.strip())
            except:
                read_count = 0
            
            try:
                comment_count = int(tds[1].text.strip())
            except:
                comment_count = 0
            
            # æ ¹æ®å†…å®¹ç±»å‹è§£æç‰¹å®šå­—æ®µ
            author = None
            grade = None
            institution = None
            notice_type = None
            publish_time_raw = ""
            
            if content_type == 'news':
                # èµ„è®¯ï¼šç¬¬4åˆ—æ˜¯ä½œè€…ï¼Œç¬¬5åˆ—æ˜¯æ—¶é—´
                try:
                    author = tds[3].a.text.strip() if tds[3].a else None
                except:
                    author = None
                publish_time_raw = tds[4].text.strip()
                
            elif content_type == 'report':
                # ç ”æŠ¥ï¼šç¬¬4åˆ—æ˜¯è¯„çº§ï¼Œç¬¬5åˆ—æ˜¯æœºæ„ï¼Œç¬¬6åˆ—æ˜¯æ—¶é—´
                try:
                    grade = tds[3].text.strip() if len(tds) > 3 else None
                    institution = tds[4].text.strip() if len(tds) > 4 else None
                    publish_time_raw = tds[5].text.strip() if len(tds) > 5 else ""
                except:
                    grade = None
                    institution = None
                    publish_time_raw = ""
                    
            elif content_type == 'notice':
                # å…¬å‘Šï¼šç¬¬4åˆ—æ˜¯å…¬å‘Šç±»å‹ï¼Œç¬¬5åˆ—æ˜¯æ—¶é—´
                try:
                    notice_type = tds[3].text.strip() if len(tds) > 3 else None
                    publish_time_raw = tds[4].text.strip() if len(tds) > 4 else ""
                except:
                    notice_type = None
                    publish_time_raw = ""
            
            # è§£æpublish_timeå¹¶æ¨æ–­å¹´ä»½
            publish_time_with_year = self._infer_year_for_publish_time(publish_time_raw)
            
            # æ–°çš„è§„èŒƒåŒ–å­—æ®µç»“æ„
            data_json = {
                "stock_code": self.secCode,                        # è‚¡ç¥¨ä»£ç 
                "content_type": content_type,                      # å†…å®¹ç±»å‹
                "title": tds[2].a.text.strip(),                   # æ ‡é¢˜
                "url": full_url,                                   # å®Œæ•´URL
                "url_id": url_id,                                  # URLå”¯ä¸€ID
                "read_count": read_count,                          # é˜…è¯»æ•°ï¼ˆæ•°å­—ï¼‰
                "comment_count": comment_count,                    # è¯„è®ºæ•°ï¼ˆæ•°å­—ï¼‰
                "publish_time": publish_time_with_year,            # å‘å¸ƒæ—¶é—´ï¼ˆå¸¦å¹´ä»½ï¼‰
                "author": author,                                  # ä½œè€…ï¼ˆä»…èµ„è®¯ï¼‰
                "grade": grade,                                    # è¯„çº§ï¼ˆä»…ç ”æŠ¥ï¼‰
                "institution": institution,                        # æœºæ„ï¼ˆä»…ç ”æŠ¥ï¼‰
                "notice_type": notice_type,                        # å…¬å‘Šç±»å‹ï¼ˆä»…å…¬å‘Šï¼‰
                "summary": tds[2].a.text.strip()[:100],           # æ‘˜è¦ï¼ˆæ ‡é¢˜å‰100å­—ï¼‰
                "crawl_time": datetime.now(),                     # çˆ¬å–æ—¶é—´
                "source": "official",                             # æ¥æºæ ‡è¯†
                "created_at": datetime.now(),                     # åˆ›å»ºæ—¶é—´
                "updated_at": datetime.now()                      # æ›´æ–°æ—¶é—´
            }
            return data_json
            
        except Exception as e:
            # é™é»˜è·³è¿‡è§£æå¤±è´¥çš„æ•°æ®
            return None


        return data_json




    @retry(
    retry=(
        retry_if_exception_type( NetworkException) |
        retry_if_exception_type(ServerException)
        # retry_if_exception(lambda e: isinstance(e, ContentChangedException))
    ),
    stop=stop_after_attempt(5) ,
    wait=wait_exponential(multiplier=1, min=2, max=4)
)
    def get_data(self, page, content_type='news'):
        """
        è·å–æŒ‡å®šé¡µé¢çš„æ•°æ®
        
        Args:
            page: é¡µç 
            content_type: 'news' (èµ„è®¯) | 'report' (ç ”æŠ¥) | 'notice' (å…¬å‘Š)
        """
        # ç±»å‹æ˜ å°„
        type_map = {
            'news': '1,f',
            'report': '2,f',
            'notice': '3,f'
        }
        
        # æ­£ç¡®çš„å®˜æ–¹èµ„è®¯/ç ”æŠ¥/å…¬å‘ŠURLæ ¼å¼
        if page == 1:
            Url = f"https://guba.eastmoney.com/list,{self.secCode},{type_map[content_type]}.html"
        else:
            type_prefix = type_map[content_type]
            Url = f"https://guba.eastmoney.com/list,{self.secCode},{type_prefix}_{page}.html"
        data_list = None
        
        try:
            self.logger.info(f"å¼€å§‹å¤„ç†é¡µé¢ {Url}")
            
            soup = self.get_soup_form_url(Url)
            if soup is None:
                raise NetworkException(f"æ— æ³•è·å–é¡µé¢ {page}")
                
            data_list = soup.find_all("tr", "listitem")
            if not data_list:
                raise NetworkException(f"é¡µé¢ {page} æ— æ•°æ®")

            new_insert_count = 0
            batch_data = []
            
            for item in data_list:
                data_json = self.get_data_json(item, content_type)
                if data_json:
                    batch_data.append(data_json)
            
            if not batch_data:
                return 0

            # ä½¿ç”¨ unordered æ‰¹é‡æ’å…¥ï¼Œé€Ÿåº¦æ›´å¿«ä¸”ä¼šè‡ªåŠ¨è·³è¿‡é‡å¤ ID
            try:
                # è¿™é‡Œçš„ self.col æ˜¯ pymongo collection å¯¹è±¡
                result = self.col.insert_many(batch_data, ordered=False)
                new_insert_count = len(result.inserted_ids)
            except BulkWriteError as e:
                # éƒ¨åˆ†æ’å…¥æˆåŠŸï¼Œéƒ¨åˆ†å› é‡å¤ ID å¤±è´¥
                new_insert_count = e.details.get('nInserted', 0)
            except Exception as e:
                self.logger.error(f"æ‰¹é‡å†™å…¥å¼‚å¸¸: {e}")
                
            return new_insert_count
        except Exception as e:
            time.sleep(random.uniform(0.5, 3))
            self.logger.error(f"soupæ•°æ®è½¬data_listå¤±è´¥: {e}")
            raise NetworkException(e)
        success_count = 0
        exist_count = 0  # åˆå§‹åŒ–ï¼Œé¿å…å¼•ç”¨é”™è¯¯
        is_retry = False
        for item in data_list:
            try:
                data_json = self.get_data_json(item, content_type)
                if data_json:
                    try:
                        exist_count = 0
        
                        	
                            
                        try:
                            self.col.insert_many([data_json]) # æ³¨æ„åŒ…è£…æˆåˆ—è¡¨
                            # self.logger.info(f"å†™å…¥æ•°æ®æˆåŠŸ: {data_json['url']}")
                            # æˆåŠŸæ’å…¥ï¼Œé‡ç½®é‡å¤è®¡æ•°å™¨
                            self.consecutive_duplicates = 0
                        except DuplicateKeyError as e:
                            # é‡å¤æ•°æ®ï¼Œè®¡æ•°å™¨+1
                            self.consecutive_duplicates += 1
                            
                            if self.secCode not in  data_json['url']  and "zssh000001" in  data_json['url']:
                                # self.logger.error(f"å¯èƒ½è¢«é‡å®šå‘: {data_json['href']}")
                                raise ContentChangedException("å†…å®¹é‡å¤")
                                is_retry = True
                            else:
                                # é™é»˜è·³è¿‡é‡å¤æ•°æ®ï¼Œä¸æ‰“å°æ—¥å¿—
                                continue
                        except BulkWriteError as e:
                            # æ£€æŸ¥æ˜¯å¦æ˜¯é‡å¤é”®é”™è¯¯
                            if self.secCode not in  data_json['url']  and "zssh000001" in  data_json['url']:
                                # self.logger.error(f"å¯èƒ½è¢«é‡å®šå‘: {data_json['href']}")
                                is_retry = True
                                raise ContentChangedException("å†…å®¹é‡å¤")
                            if e.details.get('writeErrors') and any(err.get('code') == 11000 for err in e.details['writeErrors']):
                                # é™é»˜è·³è¿‡é‡å¤æ•°æ®
                                self.consecutive_duplicates += 1
                                exist_count += 1
                                continue
                            else:
                                self.logger.error(f"æ‰¹é‡å†™å…¥å¤±è´¥: {e}")
                                continue
                        except Exception as e:
                            self.logger.error(
                                f"å†™å…¥æ•°æ®å¤±è´¥: {e}"
                            )
                            continue
                        # å†™å…¥Redisï¼ˆå¦‚æœå¯ç”¨ï¼‰
                        if self.redis_enabled and self.redis_client:
                            try:
                                self.redis_client.add_url(data_json["url"])
                            except Exception:
                                pass  # é™é»˜å¤±è´¥ï¼Œä¸åˆ·å±
                        success_count += 1
                    except Exception as e:
                        self.logger.error(f"æ’å…¥æ•°æ®å¤±è´¥: {e}, url: {data_json.get('url', 'æ— url')}")
                    # ç§»é™¤äº† self.t.set_postfixï¼Œå› ä¸ºå·²æ”¹ç”¨tqdmçš„page_bar
                    self.num_start += 1
                    
                    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°è¿ç»­é‡å¤é˜ˆå€¼
                    if self.consecutive_duplicates >= self.duplicate_threshold:
                        self.logger.warning(
                            f"è¿ç»­{self.consecutive_duplicates}æ¡é‡å¤æ•°æ®ï¼Œ"
                            f"å·²è¾¾åˆ°é˜ˆå€¼({self.duplicate_threshold})ï¼Œ"
                            f"åœæ­¢çˆ¬å–è¯¥è‚¡ç¥¨: {self.secCode}"
                        )
                        # æå‰ç»ˆæ­¢å½“å‰é¡µé¢çš„å¤„ç†
                        return -1  # è¿”å›ç‰¹æ®Šå€¼è¡¨ç¤ºæå‰ç»ˆæ­¢
                else:
                    # é™é»˜è·³è¿‡ç©ºæ•°æ®
                    pass
            except Exception as e:
                self.logger.error(f"å¤„ç†å•ä¸ª item å¤±è´¥: {e}")
        if is_retry == True:
            # é¡µé¢æ²¡æœ‰æ–°æ•°æ®ï¼Œå‡†å¤‡é‡è¯•
            self.logger.warning(f"é¡µé¢ {page} æ²¡æœ‰æ–°æ•°æ®ï¼Œå‡†å¤‡é‡è¯•")
            #æŠ›å‡º ContentChangedException é”™è¯¯
            raise ContentChangedException("å†…å®¹é‡å¤")
        
        self.logger.info(f"é¡µé¢ {page} å¤„ç†å®Œæˆï¼ŒæˆåŠŸæ’å…¥ {success_count}/{len(data_list)} æ¡æ•°æ®")
        return exist_count

 
    #æ£€æŸ¥ç´¢å¼•å­˜åœ¨æ€§
    def index_exists_by_name(self, collection, index_name):
        return index_name in collection.index_information()
    
    def _crawl_single_page(self, page: int, content_type: str) -> int:
        """
        çº¿ç¨‹æ± ä½¿ç”¨çš„å•é¡µçˆ¬å–åŒ…è£…æ–¹æ³•
        
        Args:
            page: é¡µç 
            content_type: å†…å®¹ç±»å‹
        
        Returns:
            æ–°å¢æ•°æ®æ¡æ•°ï¼Œ-1è¡¨ç¤ºè¾¾åˆ°é‡å¤é˜ˆå€¼
        """
        retry_count = 0
        max_retries = 5
        
        while retry_count < max_retries:
            try:
                new_count = self.get_data(page, content_type)
                return new_count
            except Exception as e:
                retry_count += 1
                if retry_count >= max_retries:
                    self.logger.error(f"é¡µé¢{page}é‡è¯•{max_retries}æ¬¡åä»å¤±è´¥: {e}")
                    return 0
                time.sleep(random.uniform(1, 2))
        
        return 0

                
    def main(self):
        
        # åˆ›å»ºå¤åˆå”¯ä¸€ç´¢å¼•ï¼šstock_code + content_type + url_id
        try:
            if not self.index_exists_by_name(self.col, "idx_stock_type_url"):
                self.logger.info("åˆ›å»ºå¤åˆå”¯ä¸€ç´¢å¼•: stock_code + content_type + url_id")
                self.col.create_index(
                    [("stock_code", 1), ("content_type", 1), ("url_id", 1)], 
                    unique=True, 
                    name="idx_stock_type_url"
                )
            
            # åˆ›å»ºæŸ¥è¯¢ä¼˜åŒ–ç´¢å¼•
            if not self.index_exists_by_name(self.col, "idx_stock_type_time"):
                self.logger.info("åˆ›å»ºæŸ¥è¯¢ç´¢å¼•: stock_code + content_type + crawl_time")
                self.col.create_index(
                    [("stock_code", 1), ("content_type", 1), ("crawl_time", -1)], 
                    name="idx_stock_type_time"
                )
        except Exception as e:
            self.logger.error(f"åˆ›å»ºç´¢å¼•å¤±è´¥: {e}")
            pass
         
        # ä¸‰ç§å†…å®¹ç±»å‹
        content_types = ['news', 'report', 'notice']
        type_names = {'news': 'èµ„è®¯', 'report': 'ç ”æŠ¥', 'notice': 'å…¬å‘Š'}
        
        # çˆ¬å–å‰é¢„æ£€ä»£ç†æ± å·²ç§»é™¤ï¼ˆç”±schedulerç»Ÿä¸€ç®¡ç†ï¼‰
        # if self.proxy_manager:
        #     self.proxy_manager.revalidate_pool()

        for content_type in content_types:
            # è‡ªåŠ¨æ£€æµ‹æ€»é¡µæ•°
            total_pages = self.get_total_pages(content_type)
            
            if total_pages == 0:
                print(f"âš ï¸ æ— æ³•è·å–{type_names[content_type]}é¡µæ•°ï¼Œè·³è¿‡")
                continue
            
            print(f"\n{'='*60}")
            print(f"å¼€å§‹çˆ¬å–{type_names[content_type]}ï¼Œå…±{total_pages}é¡µ")
            print(f"{'='*60}")
            
            # é‡ç½®é‡å¤è®¡æ•°å™¨å’Œå¹´ä»½æ¨æ–­çŠ¶æ€
            self.consecutive_duplicates = 0
            self.last_month = None
            self.current_year = datetime.now().year
            
            # çˆ¬å–æ‰€æœ‰é¡µé¢ - å¤šçº¿ç¨‹ç‰ˆæœ¬
            consecutive_empty_pages = 0  # è®°å½•è¿ç»­æ— æ–°æ•°æ®çš„é¡µé¢æ•°
            page_results = {}  # å­˜å‚¨æ¯é¡µçš„çˆ¬å–ç»“æœ
            
            # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘çˆ¬å–
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                # æäº¤æ‰€æœ‰é¡µé¢ä»»åŠ¡
                future_to_page = {
                    executor.submit(self._crawl_single_page, page, content_type): page
                    for page in range(1, total_pages + 1)
                }
                
                # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
                with tqdm(total=total_pages, desc=f"{type_names[content_type]}") as pbar:
                    for future in as_completed(future_to_page):
                        page = future_to_page[future]
                        try:
                            new_count = future.result()
                            page_results[page] = new_count
                            
                            # æ›´æ–°è¿›åº¦æ¡
                            if new_count == 0:
                                pbar.set_postfix({"æ–°å¢": 0, "çŠ¶æ€": "å·²åŒæ­¥"})
                            elif new_count == -1:
                                pbar.set_postfix({"çŠ¶æ€": "è¾¾åˆ°é‡å¤é˜ˆå€¼"})
                            else:
                                pbar.set_postfix({"æ–°å¢": new_count, "çŠ¶æ€": "è¿›è¡Œä¸­"})
                            
                            pbar.update(1)
                            
                        except Exception as e:
                            self.logger.error(f"é¡µé¢{page}çˆ¬å–å¤±è´¥: {e}")
                            pbar.update(1)
            
            # æ£€æŸ¥ç»“æœï¼Œåˆ¤æ–­æ˜¯å¦æå‰ç»ˆæ­¢
            # ç»Ÿè®¡è¿ç»­ä¸º0çš„é¡µé¢
            sorted_pages = sorted(page_results.keys())
            for page in sorted_pages[-5:]:  # åªæ£€æŸ¥æœ€å5é¡µ
                if page_results.get(page, 0) == 0:
                    consecutive_empty_pages += 1
                else:
                    consecutive_empty_pages = 0
            
            if consecutive_empty_pages >= 2:
                print(f"\nâ¹ï¸ æœ€åè¿ç»­{consecutive_empty_pages}é¡µæ— æ–°æ•°æ®ï¼Œ{self.secCode} {type_names[content_type]} æŠ“å–ç»“æŸ")
                        
                
                time.sleep(random.uniform(0, 1))
                self.num_start = 0


if __name__ == '__main__':
    """ç›´æ¥è¿è¡Œæ­¤æ–‡ä»¶æ—¶çš„å…¥å£"""
    import sys
    
    # è¯»å–é…ç½®
    config_path = 'config.ini'
    config = configparser.ConfigParser()
    config.read(config_path, encoding='utf-8')
    
    # ä»é…ç½®æ–‡ä»¶è¯»å–å‚æ•°
    sec_code = config.get('mainClass', 'secCode', fallback='600519')
    pages_start = config.getint('mainClass', 'pages_start', fallback=1)
    pages_end = config.getint('mainClass', 'pages_end', fallback=10)
    collection_name = config.get('mainClass', 'collectionName', fallback='stock_news')
    db_name = config.get('MongoDB', 'database', fallback='guba')
    
    print("=" * 60)
    print("ä¸œæ–¹è´¢å¯Œè‚¡å§çˆ¬è™« - å®˜æ–¹å’¨è¯¢æŠ“å– (æ–°ç‰ˆ)")
    print("=" * 60)
    print(f"è‚¡ç¥¨ä»£ç : {sec_code}")
    print(f"æŠ“å–èŒƒå›´: ç¬¬ {pages_start} é¡µåˆ°ç¬¬ {pages_end} é¡µ")
    print(f"æ•°æ®å­˜å‚¨: MongoDB - {db_name}.{collection_name}")
    print(f"æ•°æ®ç»“æ„: ç»Ÿä¸€Collection (æ‰€æœ‰è‚¡ç¥¨å…±ç”¨)")
    print("=" * 60)
    
    try:
        # åˆ›å»ºçˆ¬è™«å®ä¾‹
        crawler = guba_comments(
            config_path=config_path,
            config=config,
            secCode=sec_code,
            pages_start=pages_start,
            pages_end=pages_end,
            num_start=0,
            MongoDB=True,
            collectionName=collection_name,
            full_text=False
        )
        
        # å¼€å§‹çˆ¬å–
        print("\nå¼€å§‹çˆ¬å–...")
        crawler.main()
        
        print("\n" + "=" * 60)
        print("âœ… çˆ¬å–å®Œæˆï¼")
        print("=" * 60)
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­çˆ¬å–")
        sys.exit(0)
    except Exception as e:
        print(f"\n\nâŒ çˆ¬å–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
