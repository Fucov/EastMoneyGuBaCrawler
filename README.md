# ä¸œæ–¹è´¢å¯Œè‚¡å§çˆ¬è™« - ç”Ÿäº§ç³»ç»Ÿ v2.0

ç”Ÿäº§çº§24å°æ—¶æŒç»­çˆ¬è™«ç³»ç»Ÿï¼Œè‡ªåŠ¨çˆ¬å–æ‰€æœ‰Aè‚¡çš„èµ„è®¯ã€ç ”æŠ¥ã€å…¬å‘Šæ•°æ®

## âœ¨ ç‰¹æ€§

- **24å°æ—¶æŒç»­è¿è¡Œ** - è‡ªåŠ¨å¾ªç¯çˆ¬å–ï¼Œæ— éœ€äººå·¥å¹²é¢„
- **æ™ºèƒ½IPç®¡ç†** - Redisç¼“å­˜ä»£ç†ï¼Œä½äºé˜ˆå€¼è‡ªåŠ¨è¡¥å……
- **AkShareé›†æˆ** - è‡ªåŠ¨è·å–æ‰€æœ‰Aè‚¡ä»£ç ï¼ˆ~5000åªï¼‰
- **å¤šçº¿ç¨‹å¹¶å‘** - 12çº¿ç¨‹å¹¶å‘ï¼Œå……åˆ†åˆ©ç”¨ä»£ç†æ± 
- **å®Œæ•´æ—¥å¿—** - æŒ‰å¤©åˆ‡å‰²ï¼Œé”™è¯¯å•ç‹¬è®°å½•
- **æ•°æ®æŒä¹…åŒ–** - MongoDBå­˜å‚¨ï¼Œç»Ÿä¸€Collection

## ğŸ“ é¡¹ç›®ç»“æ„

```
Guba-Crawler/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.ini          # ç»Ÿä¸€é…ç½®æ–‡ä»¶
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ scheduler.py          # 24hè°ƒåº¦å™¨
â”‚   â”œâ”€â”€ stock_loader.py       # AkShareè‚¡ç¥¨åŠ è½½
â”‚   â””â”€â”€ proxy_manager.py      # Redisä»£ç†ç®¡ç†
â”œâ”€â”€ storage/
â”‚   â”œâ”€â”€ logger.py             # æ—¥å¿—ç³»ç»Ÿ
â”‚   â””â”€â”€ database.py           # æ•°æ®åº“å®¢æˆ·ç«¯
â”œâ”€â”€ logs/                     # æ—¥å¿—ç›®å½•
â”‚   â”œâ”€â”€ crawler.log           # çˆ¬è™«æ—¥å¿—
â”‚   â”œâ”€â”€ scheduler.log         # è°ƒåº¦æ—¥å¿—
â”‚   â””â”€â”€ error.log             # é”™è¯¯æ—¥å¿—
â”œâ”€â”€ main.py                   # ä¸»å…¥å£
â”œâ”€â”€ proxy_pool.py             # åŸä»£ç†æ± ï¼ˆå…¼å®¹ï¼‰
â”œâ”€â”€ main_class.py             # çˆ¬è™«æ ¸å¿ƒï¼ˆå¾…é›†æˆï¼‰
â””â”€â”€ full_text_CrawlerAsync.py # å…¨æ–‡çˆ¬è™«ï¼ˆä¿ç•™ï¼‰
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

```bash
# å®‰è£…ä¾èµ–
pip install pymongo redis akshare requests beautifulsoup4 lxml tqdm tenacity

# å¯åŠ¨Redisï¼ˆæ— å¯†ç ï¼‰
redis-server
```

### 2. é…ç½®æ–‡ä»¶

ç¼–è¾‘ `config/settings.ini`ï¼š

```ini
[MongoDB]
host = 10.139.197.213
database = xiaoyi_db

[Redis]
host = localhost
port = 6379
password =              # æœ¬åœ°æ— å¯†ç 

[Proxy]
min_count = 5          # IPä½äºæ­¤å€¼è‡ªåŠ¨è¡¥å……
target_count = 20      # è¡¥å……åˆ°æ­¤æ•°é‡

[Crawler]
max_workers = 12       # å¹¶å‘çº¿ç¨‹æ•°

[Scheduler]
mode = continuous      # continuous=24hè¿è¡Œ | once=å•æ¬¡
interval = 1800        # æ¯è½®é—´éš”30åˆ†é’Ÿ
stock_delay = 5        # æ¯åªè‚¡ç¥¨é—´éš”5ç§’
```

### 3. è¿è¡Œç³»ç»Ÿ

```bash
# å‰å°è¿è¡Œï¼ˆæµ‹è¯•ï¼‰
python main.py

# åå°è¿è¡Œï¼ˆç”Ÿäº§ï¼‰
nohup python main.py > /dev/null 2>&1 &

# æŸ¥çœ‹æ—¥å¿—
tail -f logs/scheduler.log
```

## ğŸ“Š æ ¸å¿ƒæ¨¡å—

### è°ƒåº¦å™¨ (scheduler.py)

24å°æ—¶å¾ªç¯è°ƒåº¦ï¼Œæ™ºèƒ½ç®¡ç†IPæ± 

```python
from core.scheduler import Crawler24HScheduler

scheduler = Crawler24HScheduler()
scheduler.run()
```

### è‚¡ç¥¨åŠ è½½ (stock_loader.py)

è‡ªåŠ¨ä»AkShareè·å–æ‰€æœ‰Aè‚¡

```python
from core.stock_loader import StockLoader

loader = StockLoader()
stocks = loader.get_all_stocks()  # ['600519', '000001', ...]
```

### ä»£ç†ç®¡ç† (proxy_manager.py)

Redisç¼“å­˜ä»£ç†ï¼Œè‡ªåŠ¨è¡¥å……

```python
from core.proxy_manager import ProxyManager

manager = ProxyManager()
proxy = manager.get_random_proxy()  # {'http': '...', 'https': '...'}
```

### æ—¥å¿—ç³»ç»Ÿ (logger.py)

æŒä¹…åŒ–æ—¥å¿—ï¼ŒæŒ‰å¤©åˆ‡å‰²

```python
from storage.logger import get_logger

logger = get_logger('my_module')
logger.info("æ­£å¸¸ä¿¡æ¯")
logger.error("é”™è¯¯ä¿¡æ¯")
```

## ğŸ”§ ä½¿ç”¨ç¤ºä¾‹

### æµ‹è¯•å•ä¸ªæ¨¡å—

```bash
# æµ‹è¯•æ—¥å¿—
python storage/logger.py

# æµ‹è¯•è‚¡ç¥¨åŠ è½½
python core/stock_loader.py

# æµ‹è¯•ä»£ç†æ± ï¼ˆéœ€è¦Redisï¼‰
python core/proxy_manager.py
```

### æŸ¥çœ‹è¿è¡ŒçŠ¶æ€

```bash
# æŸ¥çœ‹è¿›ç¨‹
ps aux | grep main.py

# æŸ¥çœ‹æ—¥å¿—
tail -100 logs/scheduler.log

# æŸ¥çœ‹é”™è¯¯
cat logs/error.log

# æŸ¥çœ‹IPæ± 
redis-cli HGETALL guba:proxies:valid
```

### åœæ­¢ç³»ç»Ÿ

```bash
# æŸ¥æ‰¾è¿›ç¨‹ID
ps aux | grep main.py

# ä¼˜é›…åœæ­¢
kill -SIGINT <PID>

# å¼ºåˆ¶åœæ­¢
kill -9 <PID>
```

## ğŸ“ˆ æ•°æ®æŸ¥è¯¢

### MongoDBæŸ¥è¯¢

```python
from database_client import DatabaseManager

db = DatabaseManager('config/settings.ini')
client = db.get_mongo_client('xiaoyi_db', 'stock_news')

# æŸ¥çœ‹æ€»æ•°
print(f"æ€»æ•°: {client.count_documents()}")

# æŸ¥çœ‹æŸåªè‚¡ç¥¨
docs = client.find({"stock_code": "600519"})
for doc in docs:
    print(doc['title'])
```

### RedisæŸ¥è¯¢

```bash
# æŸ¥çœ‹ä»£ç†æ•°é‡
redis-cli HLEN guba:proxies:valid

# æŸ¥çœ‹æ‰€æœ‰ä»£ç†
redis-cli HGETALL guba:proxies:valid

# æŸ¥çœ‹å¾—åˆ†æœ€é«˜çš„ä»£ç†
redis-cli HSCAN guba:proxies:valid 0 MATCH * COUNT 100
```

## ğŸ›¡ï¸ ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

### ä½¿ç”¨Systemdç®¡ç†

åˆ›å»º `/etc/systemd/system/guba-crawler.service`ï¼š

```ini
[Unit]
Description=Guba Crawler Service
After=network.target redis.service

[Service]
Type=simple
User=your_user
WorkingDirectory=/path/to/Guba-Crawler
ExecStart=/usr/bin/python3 main.py
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
```

å¯åŠ¨æœåŠ¡ï¼š

```bash
sudo systemctl start guba-crawler
sudo systemctl enable guba-crawler
sudo systemctl status guba-crawler
```

### æ—¥å¿—è½®è½¬

æ—¥å¿—å·²è‡ªåŠ¨æŒ‰å¤©åˆ‡å‰²ï¼Œä¿ç•™30å¤©

### ç›‘æ§å‘Šè­¦

å¯é›†æˆé’‰é’‰/ä¼ä¸šå¾®ä¿¡webhookï¼š

```python
# åœ¨scheduler.pyä¸­æ·»åŠ 
def send_alert(msg):
    import requests
    webhook = "your_webhook_url"
    requests.post(webhook, json={"text": msg})
```

## ğŸ” æ•…éšœæ’æŸ¥

### 1. Redisè¿æ¥å¤±è´¥

```bash
# æ£€æŸ¥Redisæ˜¯å¦è¿è¡Œ
redis-cli ping

# å¯åŠ¨Redis
redis-server
```

### 2. MongoDBè¿æ¥å¤±è´¥

```bash
# æ£€æŸ¥é…ç½®
cat config/settings.ini | grep MongoDB

# æµ‹è¯•è¿æ¥
python test_db_connection.py
```

### 3. IPæ± è€—å°½

```bash
# æ‰‹åŠ¨è¡¥å……ä»£ç†
python core/proxy_manager.py

# æŸ¥çœ‹ä»£ç†æ•°é‡
redis-cli HLEN guba:proxies:valid
```

### 4. æŸ¥çœ‹é”™è¯¯æ—¥å¿—

```bash
tail -50 logs/error.log
```

## ğŸ“ ç‰ˆæœ¬å†å²

### v2.0 (2026-01-23)
- âœ… é‡æ„ä¸ºç”Ÿäº§çº§æ¶æ„
- âœ… 24å°æ—¶æŒç»­è°ƒåº¦
- âœ… Redisä»£ç†ç¼“å­˜
- âœ… AkShareé›†æˆ
- âœ… å®Œæ•´æ—¥å¿—ç³»ç»Ÿ

### v1.0
- åŸºç¡€å¤šçº¿ç¨‹çˆ¬è™«
- æ–‡ä»¶ä»£ç†ç¼“å­˜
- æ‰‹åŠ¨è‚¡ç¥¨åˆ—è¡¨

## ğŸ“„ è®¸å¯è¯

MIT License

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPR

---

**Made with â¤ï¸ by Your Team**
